{
 "metadata": {
  "name": "",
  "signature": "sha256:ff5af146dddef9e742044c4804e57a3a84bbd797825b0f5a742bfcb1c95bad2f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem set 3: Parsing\n",
      "===========\n",
      "\n",
      "This problem set contains two parts. \n",
      "\n",
      "- Grammar design: try to design a CFG to parse sentences on Twitter.\n",
      "- Dependency parsing: design features to learn a high-accuracy dependency parser\n",
      "\n",
      "### Honor policy ###\n",
      "\n",
      "- Your work must be your own. Do not discuss the details of the assignment with other people. \n",
      "- You may of course help each other with understanding the ideas discussed in lecture and the readings, and with basic questions about programming in Python. For example, it is fine to discuss how arc-factored dependency parsing works, or how state-splitting works in CFGs. It is **not acceptable** to discuss how to implement a specific feature for dependency parsing, or how to handle a specific linguistic phenomenon in a CFG. It is unacceptable to share your code.\n",
      "- There are implementations and source code for many machine learning algorithms on the internet. Please write the code for this assignment on your own, without using these external resources."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from parseTwitter import evalParser\n",
      "\n",
      "import nltk\n",
      "import random\n",
      "\n",
      "def conllSeqGenerator(input_file):\n",
      "    \"\"\" return an instance generator for a filename\n",
      "\n",
      "    The generator yields lists of words and tags.  For test data, the tags\n",
      "    may be unknown.  For usage, see trainClassifier and applyClassifier below.\n",
      "\n",
      "    \"\"\"\n",
      "    cur_words = []\n",
      "    cur_tags = []\n",
      "    with open(input_file) as instances:\n",
      "        for line in instances:\n",
      "            if len(line.rstrip()) == 0:\n",
      "                if len(cur_words) > 0:\n",
      "                    yield cur_words,cur_tags\n",
      "                    cur_words = []\n",
      "                    cur_tags = []\n",
      "            else:\n",
      "                parts = line.rstrip().split()\n",
      "                cur_words.append(parts[0])\n",
      "                if len(parts)>1:\n",
      "                    cur_tags.append(parts[1])\n",
      "                else: cur_tags.append(unk)\n",
      "        if len(cur_words)>0: \n",
      "            yield cur_words,cur_tags\n",
      "#sprint conllSeqGenerator(\"oct27.clean.train\")\n",
      "\n",
      "def parseTags(tags,parser):\n",
      "    trees = []\n",
      "\n",
      "    try:\n",
      "        if float(nltk.__version__[0]) >= 3:\n",
      "            tree_gen = parser.parse(tags)\n",
      "            trees = [tree for tree in tree_gen]\n",
      "        else:\n",
      "            trees = parser.nbest_parse(tags)\n",
      "    except:\n",
      "        pass\n",
      "    return trees\n",
      "\n",
      "def evalParser(cfg,\n",
      "               input_file=\"oct27.clean.train\",\n",
      "               debug=False,\n",
      "               show_fns=False,\n",
      "               show_fps=False,\n",
      "               max_len=10,\n",
      "               preprocessor=lambda words,tags : (words,tags),\n",
      "               seed=0,\n",
      "               num_neg = 5):\n",
      "    random.seed(seed)\n",
      "    grammar = nltk.data.load(cfg,cache=False)\n",
      "    parser = nltk.ChartParser(grammar)\n",
      "    tp = 0.0 # True positive\n",
      "    fp = 0.0 # False positive\n",
      "    fn = 0.0 # False negative\n",
      "    num_parses = 0.0\n",
      "    unparsed = []\n",
      "    for words,tags in conllSeqGenerator(input_file):\n",
      "    \n",
      "        words_post,tags_post = preprocessor(words,tags)\n",
      "        \n",
      "      #  import pdb; pdb.set_trace()\n",
      "        if len(words) < max_len:\n",
      "            trees = parseTags(tags_post,parser)\n",
      "            if len(trees) == 0: \n",
      "                fn += 1\n",
      "                if show_fns:\n",
      "                    print \"No parse:\",words_post,tags_post\n",
      "            else: \n",
      "                tp += 1\n",
      "                num_parses += len(trees)\n",
      "            # \n",
      "            for _ in xrange(num_neg):\n",
      "                pairs = zip(words,tags)\n",
      "                random.shuffle(pairs)\n",
      "                words_post,tags_post = preprocessor([pair[0] for pair in pairs],[pair[1] for pair in pairs])\n",
      "                trees = parseTags(tags_post,parser)\n",
      "                if len(trees) > 0: \n",
      "                    fp += 1\n",
      "                    if show_fps:\n",
      "                        print \"False parse:\",words_post,tags_post\n",
      "\n",
      "    recall = tp / (tp + fn)\n",
      "    precision = tp / (tp + fp + 1e-6)\n",
      "    fmeasure = 2 * recall * precision / (recall + precision + 1e-6)\n",
      "    quality = {'f-measure': fmeasure, 'recall': recall, 'precision' : precision, \n",
      "               'parses-per-sent': num_parses / (1e-5+tp)}\n",
      "    return quality\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 1: CFG Design #\n",
      "\n",
      "Your first job is to design a CFG to parse Twitter. We provide some files:\n",
      "\n",
      "- **oct27.clean.train** contains some short sentences from the Twitter POS tagging dataset, avoiding some difficult tags like URLs and retweets.\n",
      "- **parseTwitter.py** contains some code for running a CFG parser on Twitter, using nltk\n",
      "- **simple.cfg** is a minimal CFG, using some ideas we've seen in class\n",
      "\n",
      "The setup is as follows:\n",
      "\n",
      "- You will see input with real sentences from Twitter, as well as scrambled sentences\n",
      "- The terminal symbols are tags, not words. See [here](http://www.ark.cs.cmu.edu/TweetNLP/annot_guidelines.pdf) for more on the tagnset.\n",
      "\n",
      "Scoring:\n",
      "\n",
      "- If your grammar produces at least one parse for a real sentence, that's a true positive; otherwise it's a false negative\n",
      "- If your grammar produces at least one parse for a scrambled sentence, that's a false negative\n",
      "- Your goal is to get a high f-measure, while producing as few parses per sentence as possible\n",
      "- My minimal grammar gets an F-measure of 9.8% (not very good!), with 1.6 parses per parsed sentence.\n",
      "- evalParser takes an optional argument \"show_fns=True\", which will show you the sentences that you are not successfully parsing, and an optional argument \"show_fps=True\", which will show scrambled sentences that were successfully parsed.\n",
      "\n",
      "We will run a bakeoff to see who can get the highest score on each deliverable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:bhatia-parminder.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "{'f-measure': 0.33962215838049065,\n",
        " 'parses-per-sent': 20.037029615914957,\n",
        " 'precision': 0.4218749934082032,\n",
        " 'recall': 0.28421052631578947}"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 1 ** (5 points): design a grammar that gets an f-measure of at least 0.3. Name it \"lastname-firstname.all.cfg\", submit it with your notebook on T-square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:bhatia-parminder.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "{'f-measure': 0.33962215838049065,\n",
        " 'parses-per-sent': 20.037029615914957,\n",
        " 'precision': 0.4218749934082032,\n",
        " 'recall': 0.28421052631578947}"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 2 ** (5 points): design a grammar that gets an f-measure of at least 0.25, with no more than 10 parses per sentence. If your previous grammar already got fewer than 10 parses per sentence, then change it to reduce the number of parses per sentence even further. Name it \"lastname-firstname.10.cfg\", and submit it on t-square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:bhatia-parminder.10.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "{'f-measure': 0.3046352929264616,\n",
        " 'parses-per-sent': 3.782607051040413,\n",
        " 'precision': 0.4107142783801022,\n",
        " 'recall': 0.24210526315789474}"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Splitting tags (7650 only) ##\n",
      "\n",
      "The Twitter POS tagset is very coarse-grained. You might be able to make a better parser if you could distinguish between subcategories of tags. In this problem, you will try to do this.\n",
      "\n",
      "- The evalParser() code takes an optional argument \"preprocessor\", which you can use to modify the tags in a sentence.\n",
      "- Your parser can then add special handling for the modified tags.\n",
      "\n",
      "For example, the function below splits the \"P\" tag to have a special tag \"2\", for the word \"to\". The grammar jacob.split.cfg can then use the tag \"2\" as a terminal symbol, slightly improving the f-measure and reducing the number of parses per sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def myPreprocess(words,tags):\n",
      "    for i,word in enumerate(words):\n",
      "        if (word.lower() == 'to'):\n",
      "            tags[i] = '2'\n",
      "        if (word.lower() == 'i' or word.lower() == 'me'):\n",
      "            tags[i] = 'I'\n",
      "        \n",
      "    return words,tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser('file:bhatia.parminder.split.cfg',preprocessor=myPreprocess)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "{'f-measure': 0.30555510445667533,\n",
        " 'parses-per-sent': 3.4999984090916323,\n",
        " 'precision': 0.4489795826738861,\n",
        " 'recall': 0.23157894736842105}"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3** (3 points; 7650 only): use tag splitting to improve your parser. \n",
      "\n",
      "- A small improvement like the one I obtained above is sufficient, but use a different split than \"to\". \n",
      "- You may check out [Klein and Manning 2003](nlp.stanford.edu/pubs/unlexicalized-parsing.pdf) for ideas. \n",
      "- Name your grammar \"lastname-firstname.split.cfg\", and submit it on T-square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser('file:bhatia.parminder.split.cfg',preprocessor=myPreprocess)\n",
      "# Answer :  Here I have added a heuristic based on Twitter Data, where \n",
      "# most of the times ,Tweet is about I and me, so I have used that to combine them into a smaller category of 'me'. Here without decrease in the accuracy I am able to get parse \n",
      "# of 3.49"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "{'f-measure': 0.30555510445667533,\n",
        " 'parses-per-sent': 3.4999984090916323,\n",
        " 'precision': 0.4489795826738861,\n",
        " 'recall': 0.23157894736842105}"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 2: Dependency parsing #\n",
      "\n",
      "In this problem, you will work with an arc-factored non-projective dependency parser, which is trained by average perceptron. If you were not confident about your own implementation of averaged structure perceptron, please take a look at the code, in the directories shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('parsing/')\n",
      "\n",
      "import dependency_parser as depp\n",
      "import dependency_features as depf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build a dependency parser with a given feature set\n",
      "dp = depp.DependencyParser(feature_function=depf.DependencyFeatures())\n",
      "dp.read_data(\"english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 801"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train your parser for ten iterations\n",
      "# These are *unlabeled* accuracies.\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.486 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.477\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.486\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.502\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.503\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.504\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.508\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.510\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.509\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.511\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.510\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You are going to create a series of subclasses of DependencyFeatures, which has features of your choice. Here is an example, which adds a feature that includes:\n",
      "\n",
      "- The part-of-speech of the head\n",
      "- The word of the modifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexFeats(depf.DependencyFeatures):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(LexFeats,self).create_arc_features(instance,h,m,add)\n",
      "        k = len(ff)\n",
      "        f = self.getF((k,instance.pos[h],instance.words[m]),add)\n",
      "        ff.append(f)\n",
      "       \n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several things to notice about this code.\n",
      "\n",
      "- You start by calling the same function, using the parent class. You can build a chain of feature functions in this way.\n",
      "- $h$ provides the index of the head word of the dependency arc\n",
      "- $m$ provides the index of the modifier word of the dependency arc\n",
      "- You can access the part of speech tags in the instance as instance.pos[i], where i indexes any word token\n",
      "- You can access the words themselves as instance.words[i], where $i$ again indexes the token\n",
      "- To create a feature, you call getF(), with two arguments:\n",
      "    - A feature tuple, which includes an index $k$, and any other information you want -- it need not be a tuple of exactly three items\n",
      "    - An argument \"add\", which you don't need to worry about (but you do need to include)\n",
      "    - Make sure to keep $k$ up-to-date. This prevents collisions in the space of features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's run it\n",
      "dp = depp.DependencyParser(feature_function=LexFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 23019"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.535 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.550\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.585 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.565\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.602 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.615 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.571\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.625 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.631 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.638 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.571\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.640 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.642 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.646 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4 ** (3 points): start by adding a feature that computes the distance between the head and the modifier, up to a maximum absolute value of 10.\n",
      "\n",
      "**Sanity check**: you should now have 23039 features. Accuracy should improve substantially."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import sign"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexDistFeats(LexFeats):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(LexDistFeats,self).create_arc_features(instance,h,m,add)\n",
      "        k = len(ff)\n",
      "        if(abs(h-m) <=10 ) :\n",
      "            f = self.getF((k,h-m), add) # your code here\n",
      "            ff.append(f)\n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=LexDistFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 23039"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.663 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.677\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.704 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.675\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.723 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.678\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.736 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.692\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.745 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.700\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.753 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.703\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.759 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.704\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.766 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.709\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.771 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.718\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.772 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.724\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 5 ** (3 points): now add a feature to LexDistFeats, which includes the POS of the modifier and the word of the head.\n",
      "\n",
      "**Sanity check**: The number of features should roughly double. (Do you see why?)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Answer : The number of cells should roughly double, because the tuples are complementary. Example: earlier in VP PP we had tuple as verb and the preposition word which was attached to this word. Now , we will have Prepostion and and word of the word. As for each earlier tuple we have new tuple and tuples are almost in the disjoint space, we should get around double features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexDistFeats2(LexDistFeats):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "           ff = super(LexDistFeats2,self).create_arc_features(instance,h,m,add)\n",
      "           k = len(ff)\n",
      "           f = self.getF((k,instance.words[h],instance.pos[m]),add)\n",
      "           ff.append(f)\n",
      "           return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=LexDistFeats2())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 44777"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.682 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.684\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.748 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.702\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.777 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.717\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.798 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.718\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.817 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.726\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Context features ## \n",
      "\n",
      "Add context features that consider the tags adjacent to the head and modifier. You may wish to consider various tag combinations, such as \n",
      "\n",
      " - $\\langle t[h], t[h-1], t[m]\\rangle$: head, head-left, modifier\n",
      " - $\\langle t[h], t[m], t[m+1]\\rangle$: head, modifier, modifier-right\n",
      " - $\\langle t[h], t[h-1], t[m], t[m+1]\\rangle$: head, head-left, modifier, modifier-right\n",
      " - etc\n",
      "\n",
      "Note that you can add more than one feature at a time within create_arc_features(). Watch out for edge cases!\n",
      "\n",
      "** Deliverable 6 ** (5 points):\n",
      "\n",
      "Describe what context feature templates you have added. How do they impact the total number of features? How does \n",
      "it impact the development and training accuracy?\n",
      "\n",
      "** Sanity check **: I added a few basic context features, and this increased dev set accuracy above 80%."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**  Answer 6 ** : Distance feature , which is one of the most useful feature was already done above. Here I started by adding feature for head ,word before head and the modifier. This clearly was an important feature as POS of head and head-1 provides more context to the dependency parsing as compared to just the head . An example for this would be Prepositional ambiguity which can better be resolved if we know  the POS of word prior to head. This feature gave equally good results for both dev and training accuracy , where the increase in the number of features was basically to the maximum for T cube , where T is the number of  Tags.\n",
      "For the same case , I have added the feature for POS for head, modifier and word on right of modifier( as wel on the left side of modifier), which provide more context from the modifiers end . An example could be Determinor Noun gives more information about the assignment of the NP to VP as compared to some PP. These features gave better results in training data as compared to dev data(though there was increase), which maybe due to overfitting of data.The increase in the number of features was basically to the maximum for T cube , where T is the number of Tags. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ContextFeats(LexDistFeats2):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "         ff = super(ContextFeats,self).create_arc_features(instance,h,m,add)\n",
      "         k = len(ff)\n",
      "         if(h>1):\n",
      "             f = self.getF((k,instance.pos[h],instance.pos[h-1],instance.pos[m]),add)\n",
      "             ff.append(f) \n",
      "         if(m < len(instance.pos)-1):\n",
      "             f1 = self.getF((k,instance.pos[h],instance.pos[m],instance.pos[m+1]),add)\n",
      "             ff.append(f1)\n",
      "         if(m>1):\n",
      "             f = self.getF((k,instance.pos[h],instance.pos[m],instance.pos[m-1]),add)\n",
      "             ff.append(f) \n",
      "         return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=ContextFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 54262"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.747 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.792\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.809 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.814\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.831 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.815\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.848 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.819\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.859 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.818\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.871 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.821\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.880 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.823\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.886 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.822\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.893 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.825\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.898 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.826\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bakeoff! ##\n",
      "\n",
      "Try to develop other features that will further improve performance.\n",
      "Please explain all the features that you have. \n",
      "After identifying your best feature set, run *test()* function from DependencyParser. This will produce a file **data/deppars/english_test.conll.pred**.\n",
      "\n",
      "**Deliverable 7** (5 points): Rename this to **lastname-firstname.conll.pred** and include it in your t-square submission.\n",
      "\n",
      "**Sanity check / challenge**: The best test set performance in Fall 2013 was 87.2%. Can you beat it??\n",
      "\n",
      "**NOTE**: As usual, you are not supposed to look at other code online while doing this problem set. However, you are welcome to search for *research papers* on dependency parsing to get ideas for features that might improve your performance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bakeoff! Answer##\n",
      "\n",
      "For the BakeOff, I have added various features, which helped in getting accuracy above 86 percent. I have added a feature for head word and word preceeding the head word. I have added another word word feature where I check the POS of modifier and add feature for all the words with same POS as the modifier with the head word. So, now even if some noun was never a modifier of verb but it came in the sentence, then this feature would accurately add the dependency in Development data.\n",
      "Then I have added tuple for head head-1,modifier,modifier+1, which gives more context to be leveraged for parsing. Atlast I have a tuple for head word, head POS,modifier word,modifier POS. Though this one is more of an overfitting feature, but it helped in further improving the accuracy of the development data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BakeOffFeats(ContextFeats):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "         ff = super(BakeOffFeats,self).create_arc_features(instance,h,m,add)\n",
      "         k = len(ff)\n",
      "         if(h>1):\n",
      "             f = self.getF((k,instance.words[h],instance.words[h-1]),add)\n",
      "             ff.append(f)\n",
      "        \n",
      "         if(h>1) and m < len(instance.pos)-1: \n",
      "             f1 = self.getF((k,instance.pos[h],instance.pos[h-1],instance.pos[m],instance.pos[m+1]),add)\n",
      "             ff.append(f1)\n",
      "         f1 = self.getF((k,instance.words[h],instance.words[m],instance.pos[h],instance.pos[m]),add)\n",
      "         ff.append(f1) \n",
      "         \n",
      "         for i in range(len(instance.words)):\n",
      "            if instance.pos[i] == instance.pos[m] :\n",
      "                 f1 = self.getF((k,instance.words[h],instance.words[i]),add)\n",
      "                 ff.append(f1) \n",
      "                            \n",
      "         return ff\n",
      "\n",
      "dp = depp.DependencyParser(feature_function=BakeOffFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 196082"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.769 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.793\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.865 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.826\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.903 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.844\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.923 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.853\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.938 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.856\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.948 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.860\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.956 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.863\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.961 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.863\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.967 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.864\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.970 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.864\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "dp.test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}