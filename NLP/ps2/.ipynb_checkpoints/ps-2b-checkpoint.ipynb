{
 "metadata": {
  "name": "",
  "signature": "sha256:f6853c7d442d0103a7633b4f8f439d48dfcdb300f5c23de16c3a41041972a5fd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem Set 2: Sequence labeling\n",
      "=====================\n",
      "\n",
      "This project focuses on sequence labeling, in the target domain of Twitter part-of-speech tagging.\n",
      "Part (b) focuses on *discriminative* approaches, mainly averaged perceptron and structured perceptron.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scorer, operator\n",
      "from collections import defaultdict, Counter\n",
      "import matplotlib.pyplot as plt\n",
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Data processing code\n",
      "\"\"\"\n",
      "def conllSeqGenerator(input_file,max_insts=1000000):\n",
      "    \"\"\" \n",
      "    return an instance generator for a filename.\n",
      "    \n",
      "    The generator yields lists of words and tags.  \n",
      "    No need to change this\n",
      "    \"\"\"\n",
      "    cur_words = []\n",
      "    cur_tags = []\n",
      "    num_insts = 0\n",
      "    with open(input_file) as instances:\n",
      "        for line in instances:\n",
      "            if len(line.rstrip()) == 0:\n",
      "                if len(cur_words) > 0:\n",
      "                    num_insts += 1\n",
      "                    yield cur_words,cur_tags\n",
      "                    cur_words = []\n",
      "                    cur_tags = []\n",
      "            else:\n",
      "                parts = line.rstrip().split()\n",
      "                cur_words.append(parts[0])\n",
      "                if len(parts)>1:\n",
      "                    cur_tags.append(parts[1])\n",
      "                else: cur_tags.append(unknown)\n",
      "        if len(cur_words)>0: \n",
      "            num_insts += 1\n",
      "            yield cur_words,cur_tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Define the file names\n",
      "trainfile = 'oct27.train'\n",
      "devfile = 'oct27.dev'\n",
      "testfile = 'oct27.test' # You do not have this for now\n",
      "unknown = \"**UNKNOWN**\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for convenience\n",
      "tr_all = []\n",
      "for i,(words,tags) in enumerate(conllSeqGenerator(trainfile)):\n",
      "    tr_all.append((words,tags))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alltags = set()\n",
      "for i,(words, tags) in enumerate(tr_all):    \n",
      "    for tag in tags:\n",
      "        alltags.add(tag)\n",
      "print alltags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['!', '#', '$', '&', ',', 'A', '@', 'E', 'D', 'G', 'M', 'L', 'O', 'N', 'P', 'S', 'R', 'U', 'T', 'V', 'Y', 'X', 'Z', '^', '~'])\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_tag = '--START--'\n",
      "end_tag = '--END--'\n",
      "trans ='--T--'\n",
      "emit = '--E--'\n",
      "offset = '--OFF--'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Classification-based tagging #\n",
      "\n",
      "First, you will perform tagging as classification problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that in structured prediction, we have the feature function decompose:\n",
      "\n",
      "\\begin{align}\n",
      "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
      "\\vec{f}(\\vec{w},\\vec{y}) & = \\sum_m \\vec{f}(\\vec{w},y_m, y_{m-1}, m)\n",
      "\\end{align}\n",
      "\n",
      "You will explicitly define your feature functions in this way -- even for the classification-based tagger, which won't consider $y_{m-1}$. The features themselves are defined as tuples, as in pset 2a.\n",
      "\n",
      "Here is a simple example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def wordFeatures(words,tag,prev_tag,m):\n",
      "    '''\n",
      "    :param words: a list of words\n",
      "    :type words: list\n",
      "    :param tag: a tag\n",
      "    :type tag: string\n",
      "    :type prev_tag: string\n",
      "    :type m: int\n",
      "    '''\n",
      " #   print offset,tag\n",
      "    out = {(offset,tag):1}\n",
      "    if m < len(words): #we can have m = M, for the transition to the end state\n",
      "        out[(emit,tag,words[m])]=1\n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = 'they can can fish'.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordFeatures(sent,'V','V',2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "{('--E--', 'V', 'can'): 1, ('--OFF--', 'V'): 1}"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4a** (1 point) Create a new feature function which also includes the final character of the current word, and the final character of the preceding word (if $m > 1$). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "curr_suffix = '--curr-suff--'\n",
      "prev_suffix = '--prev-suff--'\n",
      "def wordCharFeatures(words,tag,prev_tag,m):\n",
      "    output = wordFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "    # add your code here\n",
      "    output[(curr_suffix,tag,words[m][-1])] =1\n",
      "    if(m>0):\n",
      "         output[(prev_suffix,tag,words[m-1][-1])] =1\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check desired output\n",
      "print wordCharFeatures(sent,'V','V',1)\n",
      "# no prev-suff feature in this one, because m=0\n",
      "print wordCharFeatures(sent,'V','V',0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{('--curr-suff--', 'V', 'n'): 1, ('--E--', 'V', 'can'): 1, ('--OFF--', 'V'): 1, ('--prev-suff--', 'V', 'y'): 1}\n",
        "{('--curr-suff--', 'V', 'y'): 1, ('--E--', 'V', 'they'): 1, ('--OFF--', 'V'): 1}\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now you will define a classification-based tagger. To get you started, here are some test weights."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_weights = defaultdict(float)\n",
      "test_tags = ['N','V','V','N']\n",
      "for i in range(len(sent)):\n",
      "    for feat in wordFeatures(sent,test_tags[i],'X',i):\n",
      "        test_weights[feat] = 1\n",
      "    for feat in wordFeatures(sent,'X','X',i):\n",
      "        test_weights[feat] = 1\n",
      "print test_weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<type 'float'>, {('--E--', 'X', 'they'): 1, ('--E--', 'X', 'can'): 1, ('--E--', 'V', 'can'): 1, ('--OFF--', 'N'): 1, ('--OFF--', 'X'): 1, ('--E--', 'X', 'fish'): 1, ('--E--', 'N', 'they'): 1, ('--OFF--', 'V'): 1, ('--E--', 'N', 'fish'): 1})\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use this to find the highest-scoring label\n",
      "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4b** (1 point): Define a function that takes a list of words, feature function, dict of weights, and a tagset, and outputs a list of predicted tags (one per word)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classifierTagger(words,featfunc,weights,all_tags):\n",
      "    \"\"\"\n",
      "    :param words: list of words\n",
      "    :param features: function from lists of words and tags to list of features\n",
      "    :param weights: defaultdict of weights\n",
      "    :param all_tags: list of permissible tags\n",
      "    :returns list of tags\n",
      "    \"\"\"\n",
      "    out = []\n",
      "    for i in range(0,len(words)):\n",
      "        best = defaultdict(float)\n",
      "        for cur in all_tags:\n",
      "       #     for prev in all_tags:\n",
      "                  for feat in  featfunc(words,cur,'X',i):\n",
      "                       best[cur] =    best[cur] +  weights[feat]\n",
      "       \n",
      "        out.append( argmax(best))\n",
      "                        #     print weights[featfunc(words,cur,prev,i).keys()[0] ]\n",
      "   \n",
      "    # your code\n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check from my implementation\n",
      "classifierTagger(sent,wordFeatures,test_weights,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "['N', 'V', 'V', 'N']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's a function that evaluates a tagger on the devset. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evalTagger(tagger,outfilename,evalfile=devfile):\n",
      "    \"\"\"\n",
      "    :param tagger: a function that takes words and a list of candidate tags, and outputs a list of tags\n",
      "    :param outfilename: a filename to store the output\n",
      "    :param evalfile: a filename of the dev set data\n",
      "    \"\"\"\n",
      "    with open(outfilename,'w') as outfile:\n",
      "        for words,_ in conllSeqGenerator(evalfile):\n",
      "            pred_tags = tagger(words,alltags)\n",
      "            for tag in pred_tags:\n",
      "                print >>outfile, tag\n",
      "            print >>outfile, \"\"\n",
      "    return scorer.getConfusion(evalfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To use evalTagger, you'll need pass in a tagger which takes only two arguments. \n",
      "\n",
      "To do this, you can use a lambda expression to pass some arguments to classifierTagger, as shown below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "confusion = evalTagger(lambda words,alltags : classifierTagger(words,wordFeatures,test_weights,alltags),'testn')\n",
      "print scorer.accuracy(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.139539705577\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4c** (3 points): Apply your averaged perceptron from pset 1b to do part-of-speech tagging. Start by adapting your oneItAvgPerceptron function. You'll have to make some changes:\n",
      "\n",
      "- Replace your call to the predict() function with a call to classifierTagger()\n",
      "- The instanceGenerator now produces word lists and tag lists as instances, instead of feature counts.\n",
      "- You can treat entire sentences as instances, if you want -- this may be slightly easier. This means that you only update the weights after seeing an entire sentence, sort of like a minibatch.\n",
      "- You'll want to add the feature function as an extra argument to both oneItAvgPerceptron and trainAvgPerceptron\n",
      "- return the training accuracy rather than the number of errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def oneItAvgPerceptron(inst_generator,featfunc,weights,wsum,tagset,Tinit=0):\n",
      "    \"\"\"\n",
      "    :param inst_generator: iterator over instances\n",
      "    :param featfunc: feature function on (words, tag_m, tag_m_1, m)\n",
      "    :param weights: default dict\n",
      "    :param wsum: weight sum, for averaging\n",
      "    :param tagset: set of permissible tags\n",
      "    :param Tinit: initial value of t, the counter over instances\n",
      "    :returns weights: a defaultdict of weights\n",
      "    :returns wsum: a defaultdict of weight sums, for averaging (as in pset 1b)\n",
      "    :returns tr_acc: training set accuracy\n",
      "    :return i: a counter of the number of instances seen\n",
      "    \"\"\"\n",
      "    tr_err = 0.0\n",
      "#    t=0\n",
      "    for i,(words,y_true) in enumerate(inst_generator):\n",
      "   #     pass # your code here\n",
      "    # note that i'm computing tr_acc for you, as long as you properly update tr_err\n",
      "        predicted_labels = classifierTagger(words,featfunc,weights,tagset);\n",
      "\n",
      "        \n",
      "        j=0\n",
      "    #    prev = start_tag\n",
      "        for word in words:\n",
      "            predicted_label = predicted_labels[j]\n",
      "            label = y_true[j]\n",
      "            if label != predicted_label:\n",
      "                tr_err = tr_err + 1\n",
      "            for feat in  featfunc(words,predicted_label,'X',j):\n",
      "                    weights[feat] =  weights[feat] - 1  \n",
      "                    wsum[feat] =  wsum[feat] - (Tinit + i)\n",
      "            for feat in  featfunc(words,label,'X',j):\n",
      "                    weights[feat] =  weights[feat] + 1 \n",
      "                    wsum[feat] =  wsum[feat] +  (Tinit +i)\n",
      "       #     prev= predicted_label \n",
      "            j=j+1\n",
      "  #  print tr_err      \n",
      "  #  print sum([len(s) for s,t in inst_generator])\n",
      "    return weights, wsum, 1.-tr_err / float(sum([len(s) for s,t in inst_generator])), i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# here's how it's used. this takes approx 2 seconds for me (using %%timeit)\n",
      "weights,wsum,tr_acc,i = oneItAvgPerceptron(tr_all,wordFeatures,defaultdict(float),defaultdict(float),alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sanity check. The weight sum numbers might be different if you don't treat sentences as instances, which is what I do.\n",
      "print weights[emit,'D','the'], wsum[emit,'D','the']\n",
      "print weights[emit,'N','the'], wsum[emit,'N','the']\n",
      "print weights[emit,'V','like'], wsum[emit,'V','like']\n",
      "print weights[emit,'P','like'], wsum[emit,'P','like']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "16.0 2611.0\n",
        "-1.0 -212.0\n",
        "2.0 587.0\n",
        "5.0 942.0\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4d** (2 points): Now adapt trainAvgPerceptron function to do tagging. This should require fewer changes than oneItAvgPerceptron, but you will have to:\n",
      "\n",
      "- take a feature function as an argument\n",
      "- call evalTagger instead of evalClassifier to get the confusion matrix\n",
      "- don't forget you've modified oneItAvgPerceptron to return the training set accuracy, not the number of errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainAvgPerceptron(N_its,inst_generator,featfunc,tagset,output):\n",
      "    \"\"\"\n",
      "    :param N_its: number of iterations\n",
      "    :param inst_generator: generate words,tags pairs\n",
      "    :param featfunc: feature function\n",
      "    :param tagset: set of all possible tags\n",
      "    :returns average weights, training accuracy, dev accuracy\n",
      "    \"\"\"\n",
      "    tr_acc = [None]*N_its\n",
      "    dv_acc = [None]*N_its\n",
      "    T = 0.0\n",
      "    weights = defaultdict(float)\n",
      "    wsum = defaultdict(float)\n",
      "    for i in xrange(N_its):\n",
      "        # your code here\n",
      "        avg_weights = defaultdict(float)\n",
      "              \n",
      "        weights,wsum,tr_acc_i,j = oneItAvgPerceptron(inst_generator,featfunc,weights,wsum,tagset,T)\n",
      "        T = T + j + 1\n",
      "        for key in weights.keys():\n",
      "                 avg_weights[key] = weights[key] - ((wsum[key])/T)\n",
      "        \n",
      "        \n",
      "        confusion = evalTagger(lambda words, alltags: classifierTagger(words,featfunc,avg_weights,tagset),output)\n",
      "    #    confusion1 = evalTagger(lambda words, alltags: classifierTagger(words,featfunc,avg_weights,tagset),output,testfile)\n",
      "        if i==9:\n",
      "            print confusion\n",
      "        #    print confusion1\n",
      "        dv_acc[i] = scorer.accuracy(confusion)\n",
      "        tr_acc[i] = tr_acc_i\n",
      "        print i,'dev:',dv_acc[i],'train:',tr_acc[i]\n",
      "    return avg_weights, tr_acc, dv_acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usage example below. Note that we do about as well as the HMM with just wordCharFeatures, and we're not even considering sequence information yet. This code takes less than 30 seconds for me to run."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w, tr_acc, dv_acc = trainAvgPerceptron(10,tr_all,wordCharFeatures,alltags,'perc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.673439767779 train: 0.523428415076\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.710553597346 train: 0.685477802859\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.730043541364 train: 0.764279362473\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.736678415924 train: 0.824953827211\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.742483931163 train: 0.853615158356\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.746423387933 train: 0.879813940762\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.746423387933 train: 0.89034817703\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.746630727763 train: 0.899445926534\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.746838067593 train: 0.903892195089\n",
        "defaultdict(<type 'int'>, {('D', '^'): 2, ('#', 'R'): 1, (',', ','): 459, ('G', 'G'): 18, ('S', 'N'): 4, ('O', 'P'): 8, ('P', 'U'): 1, ('N', '@'): 6, ('&', 'R'): 1, ('N', 'R'): 8, ('G', 'P'): 1, ('V', '^'): 7, ('U', ','): 1, ('N', ','): 1, ('O', '^'): 2, ('A', 'N'): 58, ('N', 'A'): 4, ('A', 'P'): 1, ('D', 'G'): 1, ('^', ','): 1, ('V', 'N'): 67, ('O', 'O'): 304, ('~', '~'): 143, (',', '@'): 1, ('P', 'R'): 5, ('&', '&'): 87, ('D', 'L'): 3, ('E', ','): 3, ('$', 'U'): 10, ('A', '!'): 2, ('G', 'V'): 6, ('D', '!'): 3, ('^', '!'): 1, ('@', '^'): 14, ('A', '@'): 4, ('R', '^'): 2, ('L', 'V'): 5, ('A', 'R'): 11, ('L', 'D'): 2, ('#', '@'): 8, ('G', 'U'): 4, ('E', 'N'): 1, ('$', '@'): 5, ('P', 'A'): 3, ('$', '~'): 1, ('R', '&'): 1, ('N', '^'): 31, ('@', 'N'): 27, ('R', 'N'): 18, ('@', 'G'): 1, ('V', '!'): 1, ('$', 'A'): 1, ('G', '~'): 4, ('@', 'U'): 6, ('A', 'T'): 1, ('#', '^'): 5, ('$', 'P'): 5, ('^', 'D'): 2, ('#', 'U'): 3, ('T', 'N'): 1, ('E', 'V'): 2, ('$', 'N'): 3, ('P', 'V'): 4, ('G', 'O'): 4, ('N', 'N'): 508, ('P', 'D'): 1, ('O', '$'): 1, ('V', '~'): 4, ('R', 'P'): 13, ('@', 'E'): 2, ('V', 'U'): 6, ('@', 'R'): 1, ('!', '^'): 9, ('#', 'N'): 14, ('N', 'O'): 1, ('V', ','): 1, ('A', 'V'): 36, ('$', 'O'): 1, ('U', 'V'): 11, ('$', '^'): 6, ('$', '$'): 51, ('!', 'O'): 3, ('V', 'V'): 635, ('P', 'P'): 411, ('P', 'T'): 5, ('^', 'U'): 13, ('R', '@'): 2, ('V', 'E'): 1, ('^', 'G'): 4, ('X', 'R'): 2, ('R', 'R'): 143, ('!', 'N'): 19, ('A', 'O'): 2, ('@', 'P'): 1, ('~', ','): 27, ('^', 'V'): 40, ('R', 'A'): 17, ('D', 'V'): 1, ('U', 'P'): 1, ('^', '@'): 29, ('R', 'S'): 1, ('D', 'D'): 288, ('G', 'E'): 1, ('V', 'P'): 5, ('&', 'Z'): 1, ('E', 'E'): 34, ('!', ','): 2, ('O', '@'): 1, ('&', 'D'): 2, ('Z', 'N'): 6, ('U', '$'): 6, ('V', 'G'): 1, (',', '~'): 37, ('^', 'A'): 3, ('@', '@'): 153, ('R', 'T'): 2, ('!', '@'): 1, ('Z', 'V'): 3, ('A', 'A'): 112, ('@', 'V'): 29, ('!', 'R'): 2, ('L', 'N'): 8, ('U', '@'): 2, ('^', 'P'): 1, ('@', '$'): 5, ('N', '~'): 1, ('V', '@'): 16, ('E', 'U'): 3, ('V', 'R'): 2, ('N', '$'): 2, ('E', 'G'): 1, ('!', 'U'): 6, ('E', '$'): 2, ('G', 'N'): 10, ('U', '~'): 1, ('G', '^'): 3, ('R', 'D'): 2, ('G', '@'): 1, ('X', 'X'): 1, ('V', 'A'): 5, ('N', '&'): 1, ('$', '!'): 1, ('#', '#'): 5, ('R', 'V'): 6, ('L', '^'): 3, (',', 'E'): 2, ('T', 'O'): 1, ('!', '!'): 52, ('T', 'T'): 29, ('A', 'U'): 3, ('N', 'U'): 6, ('^', 'R'): 7, ('U', 'U'): 39, ('D', 'R'): 1, ('#', 'V'): 15, ('D', '@'): 2, ('E', '^'): 2, ('S', '@'): 1, ('P', 'N'): 7, ('O', 'N'): 4, ('N', 'V'): 73, ('O', 'D'): 12, ('@', 'O'): 2, (',', 'G'): 1, ('#', '!'): 1, ('!', 'D'): 2, ('N', 'E'): 5, ('L', 'U'): 1, ('U', 'E'): 1, ('!', 'V'): 3, ('T', 'R'): 1, ('D', 'P'): 4, ('U', '^'): 13, ('^', '$'): 2, ('N', '!'): 4, ('^', 'N'): 118, ('P', '^'): 1, ('R', '!'): 2, ('D', 'N'): 7, ('X', 'O'): 1, ('O', 'V'): 1, ('E', '~'): 4, ('N', 'P'): 5, ('G', 'R'): 1, ('P', '$'): 2, ('^', 'O'): 2, ('@', 'Z'): 2, ('N', 'G'): 4, ('A', '$'): 1, ('U', 'G'): 2, ('A', '^'): 8, ('L', 'L'): 46, ('T', 'P'): 4, ('U', 'N'): 14, ('$', 'V'): 2, ('^', '^'): 88, ('G', ','): 12})"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9 dev: 0.747667426913 train: 0.904713044668\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4e** (3 points): Make it better! Design a killer feature set that improves performance on the devset.\n",
      "\n",
      "I'm able to get above 84% on the dev set, without going too crazy. Warning: my additional features slow things down considerably."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "#nltk.download()\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from itertools import chain #hint, especially if you're obsessive about being pythonic\n",
      "stopset = set(stopwords.words('english'))\n",
      "lmtzr = WordNetLemmatizer()\n",
      "\n",
      "curr_suffixess = '--curr-suffess--'\n",
      "curr_suffixes = '--curr-suffes--'\n",
      "curr_prefix = '--curr-prefix--'\n",
      "prev_suffixes = '--prev-suffes--'\n",
      "curr_prefixes = '--curr-prefixes--'\n",
      "caps = '--caps--'\n",
      "thash = '--hash--'\n",
      "ing = '--ing--'\n",
      "dot = '--dot--'\n",
      "first = '--first--'\n",
      "stem = '--stem--'\n",
      "prev = '--prev--'\n",
      "def mywordFeatures(words,tag,prev_tag,m):\n",
      "    '''\n",
      "    :param words: a list of words\n",
      "    :type words: list\n",
      "    :param tag: a tag\n",
      "    :type tag: string\n",
      "    :type prev_tag: string\n",
      "    :type m: int\n",
      "    '''\n",
      " #   print offset,tag\n",
      "    out = {(offset,tag):1}\n",
      " #   out = {}\n",
      "    if m < len(words): #we can have m = M, for the transition to the end state\n",
      "  #     if words[m].isalpha():\n",
      "   #          out[(emit,tag,lmtzr.lemmatize(words[m].lower()))] =1\n",
      "    #   else:\n",
      "             out[(emit,tag,words[m].lower())]=1\n",
      "   #      out[(emit,tag, lmtzr.lemmatize(words[m].lower()))]=1\n",
      "       \n",
      "    return out\n",
      "def wordCharFeatures(words,tag,prev_tag,m):\n",
      "    output = mywordFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "    # add your code here\n",
      "    output[(curr_suffix,tag,words[m][-1])] =1\n",
      "    \n",
      "  \n",
      "             \n",
      "    if(m>0):\n",
      "         output[(prev_suffix,tag,words[m-1][-1])] =1\n",
      "    return output\n",
      "def yourFeatures(words,tag,prev_tag,m):\n",
      "    output = wordCharFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "  #  output[(prev,prev_tag)] =1  \n",
      "    if(m==0 or (m>0 and (words[m-1][-1] == '.' or words[m-1][-1] == ',') )):\n",
      "         output[(first,tag)] =1\n",
      "      \n",
      "  #  if words[m].isalpha():\n",
      "    output[(stem,tag,lmtzr.lemmatize(words[m].lower()))] =1\n",
      "    if len(words[m]) >1 :\n",
      "        output[(curr_suffixes,tag,words[m][-2] +words[m][-1])] =1\n",
      "    #    if words[m][-2:] == '..':\n",
      "     #         output[(dot,tag)] =1\n",
      "     \n",
      "        output[(curr_prefixes,tag,words[m][0] +words[m][1])] =1\n",
      "    if len(words[m]) >2 :\n",
      "        output[(curr_suffixess,tag,words[m][-3]+words[m][-2] +words[m][-1])] =1\n",
      "    if len(words[m]) >2 :\n",
      "        if words[m][-3:] == 'ing':\n",
      "              output[(ing,tag)] =1\n",
      "        if words[m][-3:] == '...':\n",
      "              output[(dot,tag)] =1\n",
      "            \n",
      "            \n",
      "#    if len(words[m-1]) >1 :    \n",
      " #          output[(prev_suffixes,tag,words[m-1][-2] +words[m-1][-1])] =1\n",
      "    #    output[(curr_suffixess,tag,words[m][-3]+words[m][-2] +words[m][-1])] =1\n",
      " #   if len(words[m]) >3:\n",
      "  #      output[(curr_suffixess,tag,words[m][-4]+words[m][-3]+words[m][-2] +words[m][-1])] =1\n",
      "     #   output[(curr_prefixes,tag,words[m][0] +words[m][1])] =1\n",
      "    output[(curr_prefix,tag,words[m][0])] =1\n",
      " #   if(m>0 and words[m][0]=='#' and words[m-1]!='.' ) :\n",
      "  #       output[(thash,tag,words[m])] =1\n",
      "    if(m>0 and words[m][0].isupper() and words[m-1]!='.' ) :\n",
      "         output[(caps,tag)] =1\n",
      "        \n",
      "#    if(m>0 and len(words[m-1])>1):\n",
      " #        output[(prev_suffixes,tag,words[m-1][-2] + words[m-1][-1])] =1\n",
      "    # your stuff\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w, tr_acc, dv_acc = trainAvgPerceptron(11,tr_all,yourFeatures,alltags,'perc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.807588637777 train: 0.693823106916\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.822102425876 train: 0.83083658253\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.827493261456 train: 0.874067993707\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.830603358905 train: 0.890142964635\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.829981339415 train: 0.901224433956\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.832262077545 train: 0.90861208017\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.829151980095 train: 0.90861208017\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.829981339415 train: 0.916820575963\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.829151980095 train: 0.918599083385\n",
        "defaultdict(<type 'int'>, {('D', '^'): 2, (',', ','): 463, ('N', '#'): 9, ('G', 'G'): 20, ('S', 'N'): 3, ('D', 'L'): 4, ('N', '@'): 1, ('N', 'R'): 5, ('G', 'P'): 2, ('V', '^'): 15, ('U', ','): 1, ('A', 'N'): 34, ('N', 'A'): 17, ('A', 'P'): 2, ('D', 'G'): 2, ('^', ','): 1, ('V', 'N'): 51, ('O', 'O'): 309, ('Z', '#'): 1, ('~', '~'): 142, ('G', 'E'): 4, ('P', 'R'): 4, ('&', '&'): 88, ('E', ','): 4, ('$', 'U'): 1, ('G', 'V'): 4, ('D', '!'): 3, ('Z', '^'): 3, ('^', '!'): 5, ('R', '^'): 4, ('A', '!'): 2, ('L', 'O'): 2, ('A', 'R'): 10, ('L', 'D'): 3, ('$', 'R'): 1, ('!', '~'): 1, ('S', 'S'): 2, ('P', 'P'): 422, ('O', 'A'): 1, ('N', 'L'): 2, ('Z', 'N'): 1, ('N', '^'): 40, ('V', '#'): 2, ('R', 'N'): 12, ('V', '$'): 1, ('^', '#'): 5, ('X', 'X'): 1, ('^', 'E'): 1, ('#', '^'): 12, ('$', 'P'): 4, ('^', 'D'): 1, ('R', 'O'): 1, ('D', 'X'): 1, ('E', 'V'): 1, ('$', 'N'): 3, ('P', 'V'): 3, ('G', 'O'): 1, ('N', 'N'): 505, ('P', 'D'): 1, ('&', 'A'): 1, ('O', 'R'): 1, ('G', '~'): 3, ('R', 'P'): 12, ('G', 'A'): 2, ('$', 'O'): 1, ('!', '^'): 4, ('#', 'N'): 4, ('N', 'O'): 1, ('A', 'V'): 28, ('$', '^'): 5, ('$', '$'): 66, ('!', 'O'): 3, ('^', 'G'): 1, ('N', '~'): 3, ('D', 'O'): 2, ('P', 'T'): 5, ('G', '$'): 1, ('^', 'U'): 1, ('V', 'E'): 1, ('$', '#'): 2, ('O', 'P'): 6, ('X', 'R'): 2, ('R', 'R'): 152, ('!', 'N'): 13, ('$', ','): 1, ('~', ','): 28, ('^', 'V'): 23, ('R', 'A'): 22, ('D', 'V'): 1, ('L', 'L'): 57, ('R', 'S'): 1, ('V', 'V'): 645, ('D', 'D'): 292, ('V', 'P'): 6, ('E', 'E'): 38, ('!', ','): 2, ('&', 'D'): 2, (',', '~'): 33, ('^', 'A'): 11, ('@', '@'): 243, ('R', 'T'): 1, ('A', 'A'): 154, ('!', 'R'): 2, ('L', 'N'): 1, ('^', 'P'): 2, ('U', 'R'): 1, ('Z', 'Z'): 4, ('V', 'R'): 4, ('N', '$'): 4, ('E', 'G'): 2, ('E', '$'): 2, ('G', 'N'): 5, ('P', 'A'): 1, ('N', 'T'): 1, ('G', '^'): 6, ('R', 'D'): 2, ('V', 'A'): 18, ('#', '#'): 35, ('$', 'A'): 1, ('T', 'O'): 1, ('!', '!'): 66, ('T', 'T'): 29, ('N', 'U'): 1, ('^', 'R'): 6, ('N', 'Z'): 1, ('U', 'U'): 87, ('D', 'R'): 1, ('#', 'V'): 1, ('^', 'L'): 3, ('E', '^'): 1, ('P', 'N'): 3, ('O', 'N'): 1, ('N', 'V'): 53, ('O', 'D'): 15, (',', 'G'): 2, ('!', 'D'): 3, ('N', 'E'): 1, ('!', 'V'): 3, ('T', 'R'): 1, ('U', 'L'): 1, (',', 'V'): 2, ('D', 'P'): 3, ('^', '$'): 3, ('N', '!'): 7, ('^', 'N'): 64, ('P', '^'): 1, ('!', 'G'): 2, ('D', 'N'): 1, ('X', 'O'): 1, ('V', 'S'): 1, ('N', 'P'): 8, ('U', 'N'): 1, ('V', '~'): 6, ('G', 'R'): 2, ('$', 'V'): 1, ('^', 'S'): 2, ('^', 'O'): 2, ('G', '#'): 2, ('L', 'Z'): 1, ('N', 'G'): 1, ('A', '$'): 1, ('L', 'A'): 1, ('A', '^'): 8, ('R', '!'): 2, ('T', 'P'): 5, ('E', '~'): 4, ('V', 'L'): 1, ('^', '^'): 180, ('G', ','): 13})"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9 dev: 0.829359319925 train: 0.918735891648\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.829359319925 train: 0.922840139544\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 5. Discriminative Structure Prediction #\n",
      "\n",
      "Now you will implement a Structured Perceptron, which is trained to find the optimal *sequence* $\\vec{y} = \\text{arg}\\max_\\vec{y} \\theta^{\\top} \\vec{f}(\\vec{w},\\vec{y})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A key difference from the classification-based setting is that we compute features over the entire sequence.\n",
      "\n",
      "**Deliverable 5a** (0.5 points): Implement a function seqFeatures, which takes a list of words, a list of tags, and a feature function, and returns a dictionary of features and their counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def seqFeatures(words,tags,featfunc):\n",
      "    '''\n",
      "    :param words: a list of words\n",
      "    :param tags: a list of tags\n",
      "    :param featfunc: a function to compute f(words,tag_m,tag_{m-1},m)\n",
      "    :returns list of features\n",
      "    '''\n",
      "    allfeats = defaultdict(float)\n",
      "    for i in range(0,len(words)):\n",
      "        if i ==0:\n",
      "            for feat in  featfunc(words,tags[i],start_tag,i):\n",
      "                       allfeats[feat] =     allfeats[feat] + 1\n",
      "        else:\n",
      "            for feat in  featfunc(words,tags[i],tags[i-1],i):\n",
      "                       allfeats[feat] =     allfeats[feat] + 1\n",
      "    for feat in  featfunc(words,end_tag,tags[i],i+1):\n",
      "                       allfeats[feat] =     allfeats[feat] + 1\n",
      "    \n",
      "    # your code here\n",
      "    return allfeats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check\n",
      "seqFeatures(sent,['N','V','V','N'],wordFeatures)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "defaultdict(<type 'float'>, {('--E--', 'V', 'can'): 2.0, ('--OFF--', 'N'): 2.0, ('--OFF--', '--END--'): 1.0, ('--E--', 'N', 'they'): 1.0, ('--OFF--', 'V'): 2.0, ('--E--', 'N', 'fish'): 1.0})"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5b** (0.5 points): now create a new feature function wordTransFeatures, which adds tag-to-tag transition features to wordFeatures. Note that this feature set is identical to what the HMM uses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def wordTransFeatures(words,tag,prev_tag,m):\n",
      "    output = wordFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "# your code here\n",
      "    if m ==0 :\n",
      "           output[(trans,tag,start_tag)] =1\n",
      " #   else if m ==len(words): \n",
      " #       output[(trans,tag,start_tag)] =1\n",
      "    else:\n",
      "           output[(trans,tag,prev_tag)] =1\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check\n",
      "test = \"dd dd dw dw k\".split()\n",
      "seqFeatures(sent,['N','V','V','N'],wordTransFeatures)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "defaultdict(<type 'float'>, {('--T--', 'V', 'V'): 1.0, ('--E--', 'V', 'can'): 2.0, ('--T--', 'N', 'V'): 1.0, ('--OFF--', 'N'): 2.0, ('--OFF--', '--END--'): 1.0, ('--T--', 'N', '--START--'): 1.0, ('--E--', 'N', 'they'): 1.0, ('--OFF--', 'V'): 2.0, ('--E--', 'N', 'fish'): 1.0, ('--T--', '--END--', 'N'): 1.0, ('--T--', 'V', 'N'): 1.0})"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5c** (1 point): copy in your viterbiTagger from part 2a. If you implemented it correctly, you should be able to use it without modification here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def viterbiTagger(words,feat_func,weights,all_tags,debug=False):\n",
      "    \"\"\"\n",
      "    :param words: list of words\n",
      "    :param feat_func: feature function\n",
      "    :param weights: defaultdict of weights\n",
      "    :param tagset: list of permissible tags\n",
      "    :param debug: optional debug flag\n",
      "    :returns output: tag sequence\n",
      "    :returns best_score: viterbi score of best tag sequence\n",
      "    \"\"\"\n",
      "    \n",
      "    trellis = [None] * (len(words) + 1) #hint: store the $v$ table here \n",
      "    pointers = [None] * (len(words) + 1) #hint: store the $b$ table here\n",
      "    output = [None] * len(words) #hint: store the output here. build this last.\n",
      "    # your code here\n",
      "    best_score = -np.inf\n",
      "    path = defaultdict(float);\n",
      "    pointer = defaultdict(float);\n",
      "    for tag in all_tags:\n",
      "        features = feat_func(words, tag,start_tag,0)\n",
      "        for feature in features:\n",
      "            path[tag] = path[tag] + weights[feature]\n",
      "  #      pointer[tag] = weights[tr]\n",
      " \n",
      "    trellis[0]= path\n",
      "  #  pointers[0]= pointer\n",
      "    \n",
      "    for i in range(1,len(words)):\n",
      "        prev = trellis[i-1]\n",
      "        cur = defaultdict(float);\n",
      "        curpointer = defaultdict(float);\n",
      "        for currentTag in all_tags:\n",
      "            max =-np.inf\n",
      "            for prevTag in all_tags:\n",
      "                features = feat_func(words, currentTag,prevTag,i)\n",
      "                sum = prev[prevTag] \n",
      "                for feature in features:\n",
      "                    sum = sum + weights[feature]\n",
      "                if sum > max :\n",
      "                    max = sum\n",
      "                #    curpointer[currentTag] = prev[prevTag] + weights[tr]\n",
      "            cur[currentTag] = max\n",
      "        trellis[i] = cur\n",
      "       # pointers[i]= curpointer\n",
      "    last = trellis[len(words) - 1]\n",
      "    end = defaultdict(float);\n",
      "   # endp = defaultdict(float);\n",
      "    for tag in all_tags:\n",
      "         features = feat_func(words, tag, end_tag, len(words))\n",
      "         end[tag] = last[tag]\n",
      "         for feature in features:\n",
      "            end[tag] = end[tag] + weights[feature]\n",
      "    #     endp[tag] = last[tag] + weights[tr[0][1],tr[0][0],tr[0][2]]  \n",
      "    trellis[len(words)]= end\n",
      "   # pointers[len(words)]= endp\n",
      "#    print trellis\n",
      "#    print pointers\n",
      "    be = trellis[len(words)-1]\n",
      "    for  tag in all_tags: \n",
      "        if best_score < be[tag]:\n",
      "            best_score =be[tag]\n",
      "    \n",
      "    for rev in range(len(words)-1,-1,-1):\n",
      "        cur = trellis[rev]\n",
      "        best = -np.inf\n",
      "        bt = ''\n",
      "        for  tag in all_tags:\n",
      "            if best < cur[tag]:\n",
      "                best = cur[tag]\n",
      "                bt = tag\n",
      "      #  print best\n",
      "        output[rev]=bt       \n",
      "        \n",
      "    return output,best_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sanity check\n",
      "viterbiTagger(['they','can','can','fish'],wordTransFeatures,test_weights,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "(['N', 'V', 'V', 'N'], 8.0)"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5d** (5 points): create a function oneItAvgStructPerceptron, which performs a single iteration of averaged structured perceptron. It should be similar to your oneItAvgPerceptron, but will have to be different in some ways to reflect the structured prediction scenario.\n",
      "\n",
      "- To make predictions, you must call your viterbiTagger function\n",
      "- To compute the features for a given sequence of words and tags, you must call you seqFeatures function\n",
      "- As above, output the training accuracy, not the number of training errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def oneItAvgStructPerceptron(inst_generator,\n",
      "                             featfunc,\n",
      "                             weights,\n",
      "                             wsum,\n",
      "                             tagset,\n",
      "                             Tinit=0):\n",
      "    \"\"\"\n",
      "    :param inst_generator: A generator of (words,tags) tuples\n",
      "    :param tagger: A function from (words, weights) to tags\n",
      "    :param features: A function from (words, tags) to a dict of features and weights\n",
      "    :param weights: A defaultdict of weights\n",
      "    :param wsum: A defaultdict of weight sums\n",
      "    :param Tinit: the initial value of the $t$ counter at the beginning of this iteration\n",
      "    :returns weights: a defaultdict of weights\n",
      "    :returns wsum: a defaultdict of weight sums, for averaging\n",
      "    :returns tr_acc: the training accuracy\n",
      "    :returns i: the number of instances (sentences) seen\n",
      "    \"\"\"\n",
      "    tr_err = 0.\n",
      "    tr_tot = 0.\n",
      "    for i,(words,y_true) in enumerate(inst_generator):\n",
      "        predicted_labels = viterbiTagger(words,featfunc,weights,tagset)[0];\n",
      "\n",
      "        tr_tot = tr_tot + 1\n",
      "        j=0\n",
      "        prev = start_tag\n",
      "        gold = seqFeatures(words, y_true,featfunc)\n",
      "        output = seqFeatures(words, predicted_labels,featfunc)\n",
      "    #    print gold\n",
      "        err = False\n",
      "        for word in words:\n",
      "            predicted_label = predicted_labels[j]\n",
      "            label = y_true[j]\n",
      "            if label != predicted_label:\n",
      "                tr_err = tr_err + 1\n",
      "                err =True\n",
      "  \n",
      "        for key in gold.keys():\n",
      "         #        if not(key in output):\n",
      "                     weights[key] =  weights[key] + gold[key]  \n",
      "                     wsum[key] = wsum[key] + (Tinit+i )*gold[key]\n",
      "        for key in output.keys():\n",
      "          #      if not(key in gold):\n",
      "                     weights[key] =  weights[key] - output[key]  \n",
      "                     wsum[key] = wsum[key] - (Tinit+i )*output[key] \n",
      "  #      print weights\n",
      "   #     break;\n",
      "        \n",
      "            \n",
      "    tr_tot = float(sum([len(s) for s,t in inst_generator]))\n",
      "        \n",
      "      # your code\n",
      "    return weights, wsum,  1.-tr_err /tr_tot, i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sanity check. Note that I'm only considering the first 100 sentences, and it still takes 6 seconds. You may want to debug on an even smaller subset of the corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights,wsum,tr_acc,i = oneItAvgStructPerceptron(tr_all[:100],wordTransFeatures,defaultdict(float),defaultdict(float),alltags)\n",
      "#%%timeit\n",
      "#weights,wsum,tr_acc,i = oneItAvgStructPerceptron(tr_all,wordTransFeatures,defaultdict(float),defaultdict(float),alltags)\n",
      "#print tr_acc\n",
      "#print weights "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sample output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tag1 in list(alltags)[:7]:\n",
      "    for tag2 in list(alltags)[:7]:\n",
      "        if weights[trans,tag1,tag2] != 0:\n",
      "            print tag1,tag2,weights[(trans,tag1,tag2)],wsum[trans,tag1,tag2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "! ! -29.0 18.0\n",
        "! , 6.0 306.0\n",
        "! @ 9.0 251.0\n",
        "$ $ 1.0 31.0\n",
        "$ , 1.0 52.0\n",
        "$ A -2.0 -102.0\n",
        "$ @ -2.0 -28.0\n",
        "& $ -1.0 -22.0\n",
        "& , 1.0 -10.0\n",
        "& @ 1.0 93.0\n",
        ", ! 12.0 574.0\n",
        ", # 1.0 19.0\n",
        ", $ 1.0 57.0\n",
        ", & -4.0 -184.0\n",
        ", , -10.0 -444.0\n",
        ", @ -2.0 -203.0\n",
        "A $ -1.0 -62.0\n",
        "A , -1.0 -13.0\n",
        "A A -4.0 -298.0\n",
        "A @ -2.0 -86.0\n",
        "@ ! 1.0 95.0\n",
        "@ $ -3.0 -42.0\n",
        "@ & 1.0 45.0\n",
        "@ , -3.0 -221.0\n",
        "@ A -1.0 -35.0\n",
        "@ @ -5.0 -93.0\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5e** (2 points): Implement trainAvgStructPerceptron. This will be quite similar to your trainAvgPerceptron from ps1b, but will have to take slightly different arguments to handle the structured prediction case. Don't forget to use evalTagger to produce output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mywordFeatures(words,tag,prev_tag,m):\n",
      "    '''\n",
      "    :param words: a list of words\n",
      "    :type words: list\n",
      "    :param tag: a tag\n",
      "    :type tag: string\n",
      "    :type prev_tag: string\n",
      "    :type m: int\n",
      "    '''\n",
      " #   print offset,tag\n",
      "    out = {(offset,tag):1}\n",
      " #   out = {}\n",
      "    if m < len(words): #we can have m = M, for the transition to the end state\n",
      "        out[(emit,tag,words[m].lower())]=1\n",
      "   #      out[(emit,tag, lmtzr.lemmatize(words[m].lower()))]=1\n",
      "       \n",
      "    return out\n",
      "def wordTransFeatures(words,tag,prev_tag,m):\n",
      "    output = mywordFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "# your code here\n",
      "    if m ==0 :\n",
      "           output[(trans,tag,start_tag)] =1\n",
      " #   else if m ==len(words): \n",
      " #       output[(trans,tag,start_tag)] =1\n",
      "            \n",
      "    else:\n",
      "           output[(trans,tag,prev_tag)] =1\n",
      "    if m< len(words):\n",
      "        output[(curr_suffix,tag,words[m][-1])] =1\n",
      "    return output\n",
      "def trainAvgStructPerceptron(N_its,inst_generator,featfunc,tagset):\n",
      "    \"\"\"\n",
      "    :param N_its: number of iterations\n",
      "    :param inst_generator: A generator of (words,tags) tuples\n",
      "    :param tagger: A function from (words, weights) to tags\n",
      "    :param features: A function from (words, tags) to a dict of features and weights\n",
      "    :returns weights: defaultdict of weights\n",
      "    :returns tr_acc: training accuracy\n",
      "    :returns dv_acc: dev set accuracy\n",
      "    \"\"\"\n",
      "\n",
      "   \n",
      "    tr_acc = [None]*N_its\n",
      "    dv_acc = [None]*N_its\n",
      "    T = 0.0\n",
      "    weights = defaultdict(float)\n",
      "    wsum = defaultdict(float)\n",
      "    for i in xrange(N_its):\n",
      "        # your code here\n",
      "        # note that I call evalTagger to produce the dev set results\n",
      "        avg_weights = defaultdict(float)\n",
      "              \n",
      "        weights,wsum,tr_acc_i,j = oneItAvgStructPerceptron(inst_generator,featfunc,weights,wsum,tagset,T)\n",
      "        T = T + j+1\n",
      "       # if T!=0:\n",
      "        for key in weights.keys():\n",
      "                 avg_weights[key] = weights[key] - ((wsum[key])/T)\n",
      "        #else:\n",
      "         #    for key in weights.keys():\n",
      "          #           avg_weights[key] = weights[key] \n",
      "            \n",
      "        confusion = evalTagger(lambda words,tags : viterbiTagger(words,featfunc,avg_weights,tags)[0],'sp.txt')\n",
      "        dv_acc[i] = scorer.accuracy(confusion)\n",
      "        tr_acc[i] = tr_acc_i#1. - tr_err/float(sum([len(s) for s,t in inst_generator]))\n",
      "        print i,'dev:',dv_acc[i],'train:',tr_acc[i]\n",
      "   \n",
      "    return weights, tr_acc, dv_acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code should roughly reproduce this sanity check. It may be a little slow, so we'll just test on the first 50 instances.\n",
      "# While you're debugging your code, you can run on even smaller datasets.\n",
      "theta,tr_acc,dv_acc = trainAvgStructPerceptron(5,tr_all[:50],wordTransFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.385444743935 train: 0.277165354331\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.450134770889 train: 0.475590551181\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.468380675928 train: 0.577952755906\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.483309143687 train: 0.804724409449\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.490358697906 train: 0.905511811024\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run your code on all the training data, using wordTransFeatures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta,tr_acc,dv_acc = trainAvgStructPerceptron(20,tr_all,wordTransFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.66701223305 train: 0.545454545455\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.685880157578 train: 0.640262671865\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.696869168567 train: 0.725494219851\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.702882023637 train: 0.763663725289\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.707443499896 train: 0.803201313359\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.707650839726 train: 0.82577467679\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.706199460916 train: 0.836514125453\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.706614140576 train: 0.857993022779\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.707236160066 train: 0.859976742595\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.705992121086 train: 0.868800875573\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.706614140576 train: 0.879540324236\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.708480199046 train: 0.878787878788\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.708687538876 train: 0.884260209317\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.707443499896 train: 0.885423079554\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.708894878706 train: 0.889116902661\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.708065519386 train: 0.894999657979\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.707650839726 train: 0.90779123059\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.707236160066 train: 0.901498050482\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.707028820236 train: 0.907722826459\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.705162761767 train: 0.90437102401\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5f** (3 points): Implement a better feature set for structured prediction. For speed reasons, you might not want to use all the features you used in 4e, but try to get as good an accuracy as you can. Last year I was able to get my structured perceptron to work a little better than my best classifier, but this year my classifier is (slightly) better!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def yourHMMFeatures(words,tag,prev_tag,m):\n",
      "    output = wordTransFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "    if m< len(words):\n",
      "     #   output[(curr_suffix,tag,words[m][-1])] =1\n",
      "        if(m>0):\n",
      "             output[(prev_suffix,tag,words[m-1][-1])] =1\n",
      "        if len(words[m]) >1 :\n",
      "            output[(curr_suffixes,tag,words[m][-2] +words[m][-1])] =1\n",
      "            output[(curr_prefixes,tag,words[m][0] +words[m][1])] =1\n",
      "        output[(curr_prefix,tag,words[m][0])] =1\n",
      "      \n",
      "        if len(words[m]) >2 :\n",
      "            output[(curr_suffixess,tag,words[m][-3]+words[m][-2] +words[m][-1])] =1\n",
      "        if len(words[m]) >2 :\n",
      "            if words[m][-3:] == 'ing':\n",
      "              output[(ing,tag)] =1\n",
      "\n",
      "        if(m>0 and words[m][0]=='#' and words[m-1]!='.' ) :\n",
      "             output[(thash,tag,words[m])] =1\n",
      "        if(m>0 and words[m][0].isupper() and words[m-1]!='.' ) :\n",
      "             output[(caps,tag)] =1\n",
      "\n",
      "        if(m>0 and words[m][0]=='#' and words[m-1]!='.' ) :\n",
      "             output[(thash,tag,words[m])] =1\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta,tr_acc,dv_acc = trainAvgStructPerceptron(9,tr_all,yourHMMFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.77441426498 train: 0.769820097134\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.786025295459 train: 0.848826869143\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.796806966618 train: 0.89458923319\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.800539083558 train: 0.922566523018\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.800746423388 train: 0.915999726383\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.801575782708 train: 0.937615431972\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.799917064068 train: 0.933990012997\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-43-1b43f7024794>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtr_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdv_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainAvgStructPerceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtr_all\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myourHMMFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malltags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-39-c779fbb44254>\u001b[0m in \u001b[0;36mtrainAvgStructPerceptron\u001b[1;34m(N_its, inst_generator, featfunc, tagset)\u001b[0m\n\u001b[0;32m     60\u001b[0m           \u001b[1;31m#           avg_weights[key] = weights[key]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mconfusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevalTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtags\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mviterbiTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sp.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mdv_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mtr_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_acc_i\u001b[0m\u001b[1;31m#1. - tr_err/float(sum([len(s) for s,t in inst_generator]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-16-6f062d91c7dd>\u001b[0m in \u001b[0;36mevalTagger\u001b[1;34m(tagger, outfilename, evalfile)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconllSeqGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mpred_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malltags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_tags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[1;33m>>\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-39-c779fbb44254>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(words, tags)\u001b[0m\n\u001b[0;32m     60\u001b[0m           \u001b[1;31m#           avg_weights[key] = weights[key]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mconfusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevalTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtags\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mviterbiTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sp.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mdv_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mtr_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_acc_i\u001b[0m\u001b[1;31m#1. - tr_err/float(sum([len(s) for s,t in inst_generator]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-33-11b548834306>\u001b[0m in \u001b[0;36mviterbiTagger\u001b[1;34m(words, feat_func, weights, all_tags, debug)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mmax\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprevTag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_tags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeat_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrentTag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprevTag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprevTag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-42-3733043587d4>\u001b[0m in \u001b[0;36myourHMMFeatures\u001b[1;34m(words, tag, prev_tag, m)\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[1;31m#   output[(curr_suffix,tag,words[m][-1])] =1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m              \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_suffix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_suffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 6. Error analysis #\n",
      "\n",
      "**Deliverable 6** (5 points; CS 7650 only). The scorer.py script produces a confusion matrix, which shows the most common types of errors. Consider your best tagger in any part of the assignment, and identify the three most frequent errors (e.g., N classified as V). Find an example sentence in your tagger has made each type of error, and explain why you think it made the mistake, and how it could be fixed. (If you are feeling competitive, you can then use this information to go back and try to improve your features.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Answer 6. Error Analysis! #\n",
      "\n",
      "\n",
      "Some of the most high Frequency errorswhich the classifier makes are as follows\n",
      "\n",
      "Proper noun classified as  noun 118.\n",
      "Proper noun classified as @ 29.\n",
      "Adjective classified as noun 58.\n",
      "verb classified as Noun 67.\n",
      "adjective as verb 36.\n",
      " \n",
      "Lets look at the example below.\n",
      "\n",
      "Senate\t^\n",
      "'#ArtsGrades'\tN\n",
      "are\tV\n",
      "in\tP\n",
      "!\t,\n",
      "See\tV\n",
      "who\tO\n",
      "passed\tV\n",
      "and\t&\n",
      "who\tO\n",
      "made\tV\n",
      "the\tD\n",
      "Dirty\t^\n",
      "Dozen\t^\n",
      ".\t,\n",
      "'#arts\t#'\n",
      "'http://t.co/BAh2iUL'\tU\n",
      "via\tP\n",
      "@ArtsActionFund\t@ .\n",
      "\n",
      "Actual Value\n",
      "N\n",
      "N\n",
      "V\n",
      "P\n",
      ",\n",
      "V\n",
      "O\n",
      "V\n",
      "&\n",
      "O\n",
      "V\n",
      "D\n",
      "N\n",
      "N\n",
      ",\n",
      "N\n",
      "^\n",
      "P\n",
      "V\n",
      "\n",
      "\n",
      "Here we can see the most of the times are when Most of the Proper Nouns are classified as Nouns(Common). As well as # and @ at the start and at the end have more chances of being of the category # or @ and when they come in the middle they have more changes of being Nouns, Verbs etc. As we have taken the last alphabet of word, so there is probability of words more falling into noun. Example: '#arts' was taken as noun because it ends with 's'. Similarly Dirty was taken as Noun instead of proper noun. There are few features which if added can improve the system. Check for the first letter of the word.(improved results by 3.1 percent). Check if special twitter symbols come in the begining or at the end. Check if word starts with Capital alphabet. For adjectives check if word ends with 'le','able'and verbs ends with 'ing' etc. Some of the suggestions are also incorporated from RatnaParkhi's Paper (1996). I have also applied lametization/stemming so that more words can fall in same category , thus helping in increasing their relative weights."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 7. Bakeoff! #\n",
      "\n",
      "48 hours before the assignment is due, we will send you unlabeled test data. Your job is to produce a response file that I can evaluate. I'll present the results in class and give the best scorers a chance to explain what they did.\n",
      "\n",
      "** Deliverable 7 ** (3 points) Run your best system from any part of the assignment on the test data. You may also try to improve your system in other ways, such as by using Passive-Aggressive to learn the weights; however, in Deliverable 5f you specifically need to improve the system by designing better features.\n",
      "\n",
      "Rename your response file as lastname-firstname.response, and include it in your submission on T-Square. (Please get this filename right, otherwise we may miss your submission to the bakeoff.) The top scores will be announced inclass.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "#w, tr_acc, dv_acc = trainAvgPerceptron(10,tr_all,yourFeatures,alltags,'perc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-144-8b41dfe302a8>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-144-8b41dfe302a8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    devtags =\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainAvgPerceptronc(N_its,inst_generator,featfunc,tagset,output):\n",
      "    \"\"\"\n",
      "    :param N_its: number of iterations\n",
      "    :param inst_generator: generate words,tags pairs\n",
      "    :param featfunc: feature function\n",
      "    :param tagset: set of all possible tags\n",
      "    :returns average weights, training accuracy, dev accuracy\n",
      "    \"\"\"\n",
      "    tr_acc = [None]*N_its\n",
      "    dv_acc = [None]*N_its\n",
      "    T = 0.0\n",
      "    weights = defaultdict(float)\n",
      "    wsum = defaultdict(float)\n",
      "    for i in xrange(N_its):\n",
      "        # your code here\n",
      "        avg_weights = defaultdict(float)\n",
      "              \n",
      "        weights,wsum,tr_acc_i,j = oneItAvgPerceptron(inst_generator,featfunc,weights,wsum,tagset,T)\n",
      "        T = T + j + 1\n",
      "        for key in weights.keys():\n",
      "                 avg_weights[key] = weights[key] - ((wsum[key])/T)\n",
      "        \n",
      "        \n",
      "        confusion = evalTagger(lambda words, alltags: classifierTagger(words,featfunc,avg_weights,tagset),output,testfile)\n",
      "    #    if i==9:\n",
      "   #         print confusion\n",
      "   #     dv_acc[i] = scorer.accuracy(confusion)\n",
      "        tr_acc[i] = tr_acc_i\n",
      "     #   print i,'dev:',dv_acc[i],'train:',tr_acc[i]\n",
      "    return avg_weights, tr_acc, dv_acc\n",
      "\n",
      "w, tr_acc, dv_acc = trainAvgPerceptronc(10,tr_all,yourFeatures,alltags,'bhatia-parminder.response')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}