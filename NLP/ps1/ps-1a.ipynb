{
 "metadata": {
  "name": "",
  "signature": "sha256:d9014bc84601232b0cd5c777c6517fc7af72597018e7661e00d14ada0af219bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem Set 1a: Text classification\n",
      "==============\n",
      "\n",
      "(_This problem set is graded out of 30 for students taking CS4650 and out of 35 for students taking CS7650. But like all problem sets, this will count towards 6% of your final grade _)\n",
      "\n",
      "\n",
      "In this problem set, you will build a system for classifying movie reviews as positive, negative, or neutral. You will:\n",
      "\n",
      "- Do some basic text processing, tokenizing your input and converting it into a bag-of-words representation\n",
      "- Build a classifier based on sentiment word lists\n",
      "- Build a machine learning classifier, using Naive Bayes\n",
      "- Evaluate your classifiers and examine what they have learned\n",
      "\n",
      "In Problem Set 1b (due one week later), you will try some more advanced classifiers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Installation ##\n",
      "\n",
      "You may need to install some of the libraries below. Usually this is done with pip or easy_install. See here:\n",
      "\n",
      "- [NLTK](http://www.nltk.org/install.html)\n",
      "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
      "- [numpy](http://docs.scipy.org/doc/numpy/user/install.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize, sent_tokenize \n",
      "from collections import defaultdict, Counter\n",
      "from matplotlib import pyplot as plt\n",
      "import numpy as np\n",
      "import scorer\n",
      "import operator\n",
      "import nltk.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 266
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Data Processing #\n",
      "(_Completing docsToBOWs() - 4 pts, each question in Deliverable 1 is worth 1 pt. Total 8 pts for part 1_)\n",
      "\n",
      "Your first step is to write code that can apply the following\n",
      "preprocessing steps. You will have to run this code fairly quickly on\n",
      "the test data when you receive it, so make sure it is modular and\n",
      "well-written.\n",
      "\n",
      "- You will edit a function that takes as its argument a \"key\" document.\n",
      "  It should produce a \"BOW\" (bag-of-words) document.\n",
      "  Each line of the key document contains a filename and a label.\n",
      "  Each line of the BOW document should contain a BOW representation of the corresponding\n",
      "  file in the key document. \n",
      "- A BOW representation looks like this: \"word:count word:count word:count...\" for every word that appears in\n",
      "  the document. Do not print words that have zero count. Use space delimiters.\n",
      "- Use NLTK's [tokenization package](http://nltk.org/api/nltk.tokenize.html) function \n",
      "  to divide each file into sentences, and each sentence into tokens.\n",
      "- Downcase all tokens\n",
      "- Only consider tokens that are completely alphabetic.\n",
      "\n",
      "I have provided some shell code, but you will have to fill in the tokenization and filtering steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def docsToBOWs(keyfile):\n",
      "     with open(keyfile,'r') as keys:\n",
      "         with open(keyfile.replace('.key','.bow'),'w') as outfile:\n",
      "            for keyline in keys:\n",
      "                dataloc = keyline.rstrip().split(' ')[0]\n",
      "             #   fcounts = ... #add code here\n",
      "                fcounts = Counter()\n",
      "                with open(dataloc,'r') as infile:\n",
      "                    for line in infile:\n",
      "                        myTokenizer(line,fcounts)\n",
      "                for word,count in fcounts.items():\n",
      "                    print >>outfile,\"{}:{}\".format(word,count), #write the word and its count to a line\n",
      "                print >>outfile,\"\"\n",
      "        \n",
      "def myTokenizer(line,fcounts):\n",
      "    sent_tokenize_list =sent_tokenize(line);\n",
      "    for sentence in sent_tokenize_list:\n",
      "            words = word_tokenize(sentence)\n",
      "            for word in words:\n",
      "       #             print word.lower()\n",
      "                    if word.isalpha():\n",
      "                        fcounts[word.lower()] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 267
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the keyfiles that are relevant to this homework. \n",
      "At the beginning you won't have test-imdb.key"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainkey = 'train-imdb.key'\n",
      "devkey = 'dev-imdb.key'\n",
      "testkey = 'test-imdb.key'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 268
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, run these lines to produce the BOW files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "docsToBOWs(trainkey)\n",
      "docsToBOWs(devkey)\n",
      "#my_tokenization()\n",
      "# docsToBOWs('test-imdb.key') # you won't have this file yet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 269
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next cell defines a [generator function](http://wiki.python.org/moin/Generators), called \"dataIterator\"\n",
      "\n",
      "- This allows you to easily iterate through the dataset defined by a given keyfile. \n",
      "- Each time you call \"next\" (possibly implicitly), it returns a dict containing features and counts for the next document in the sequence. \n",
      "- In this case, the features include the words, and a special \"offset\" feature\n",
      "- This is equivalent to $\\boldsymbol{x}_i$ in the reading.\n",
      "- You can see how this is used in the getAllCounts() function below, which takes a dataIterator as an argument.\n",
      "\n",
      "Lines 7-8 of the code might look confusing if you are not a pythonista. \n",
      "\n",
      "- This is a [list comprehension](http://legacy.python.org/dev/peps/pep-0202/)\n",
      "nested inside a [dict comprehension](http://legacy.python.org/dev/peps/pep-0274/).\n",
      "- Here's an [introduction](http://carlgroner.me/Python/2011/11/09/An-Introduction-to-List-Comprehensions-in-Python.html) with more examples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "offset = '**OFFSET**'\n",
      "def dataIterator(keyfile):\n",
      "    with open(keyfile.replace('key','bow'),'r') as bows:\n",
      "        with open(keyfile,'r') as keys:\n",
      "            for keyline in keys:\n",
      "                textloc,label = keyline.rstrip().split(' ')\n",
      "                fcounts = {word:int(count) for word,count in\\\n",
      "                           [x.split(':') for x in bows.readline().rstrip().split(' ')]}\n",
      "                fcounts[offset] = 1\n",
      "                yield fcounts,label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 270
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataIterator above incrementally re-reads the keyfile and BOW file every time you call it. \n",
      "This is a good idea if you have huge data that won't fit in memory, but the file I/O involves some overhead.\n",
      "If you want, you can write a second dataIterator that iterates across data stored in memory, which\n",
      "will be faster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: How many unique words appear in the training set? (Types, not tokens.) I get 24861. (Don't count the offset feature.)\n",
      "\n",
      "- Note how the dataIterator function is used here. \n",
      "- fcounts is a dict, which it returns for each document.\n",
      "- We are currently ignoring the label, but that is also provided.\n",
      "- This may take a couple of minutes to run"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getAllCounts(datait):\n",
      "    allcounts = Counter()\n",
      "    for fcounts, _ in datait:\n",
      "    #    print fcounts\n",
      "        allcounts += Counter(fcounts)\n",
      "    return allcounts\n",
      "\n",
      "ac_train = getAllCounts(dataIterator('train-imdb.key'))\n",
      "ac_dev = getAllCounts(dataIterator('dev-imdb.key'))\n",
      "#ac_test = getAllCounts(dataIterator('test-imdb.key'))\n",
      "print \"number of word types\",len(ac_train.keys())-1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of word types 24861\n"
       ]
      }
     ],
     "prompt_number": 271
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code makes a plot, with the log-rank (from 1 to the log of the total number of words) \n",
      "on the x-axis and the log count on the y-axis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this enables you to create inline plots in the notebook \n",
      "train_count = 0.0\n",
      "train_unique_count  = 0.0\n",
      "dev_unique_count  = 0.0\n",
      "dev_words = []\n",
      "     \n",
      "for key in ac_train.keys():\n",
      "    train_count =train_count+ac_train.get(key)\n",
      "    if(ac_train.get(key) ==1):\n",
      "        train_unique_count= train_unique_count+1\n",
      "   \n",
      "print \"number of train word tokens\", train_count\n",
      "print \"number of  dev word types\",len(ac_dev.keys())-1\n",
      "dev_count = 0.0\n",
      "\n",
      "for key in ac_dev.keys():\n",
      "    dev_count =dev_count+ac_dev.get(key)\n",
      "    if(ac_dev.get(key) ==1):\n",
      "        dev_unique_count= dev_unique_count+1\n",
      "    if(ac_train.get(key)==None):\n",
      "         dev_words.append(key)\n",
      "   \n",
      "print \"number of dev word tokens\", dev_count\n",
      "\n",
      "print \"Train ratio\", train_count/(len(ac_train.keys())-1)\n",
      "print \"Dev ratio\", dev_count/(len(ac_dev.keys())-1)\n",
      "print \"Dev single Count words\",dev_unique_count\n",
      "print \"Test single count words\",train_unique_count\n",
      "print \"Words unique in dev \" , len(dev_words)\n",
      "   \n",
      "   #     dev_words = []\n",
      "    #     if(ac_train.get(key)==0):\n",
      "     #   dev_words.append(key)\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of train word tokens 466337.0\n",
        "number of  dev word types 16900\n",
        "number of dev word tokens 223858.0\n",
        "Train ratio 18.7577732191\n",
        "Dev ratio 13.246035503\n",
        "Dev single Count words 8012.0\n",
        "Test single count words 10807.0\n",
        "Words unique in dev  4796\n",
        "Populating the interactive namespace from numpy and matplotlib"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['mat', 'argmax']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 272
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr_logcounts = np.log(np.array(sorted(ac_train.values(),reverse=True)))\n",
      "plt.plot(np.log(range(len(tr_logcounts))),tr_logcounts)\n",
      "dv_logcounts = np.log(np.array(sorted(ac_dev.values(),reverse=True)))\n",
      "plt.plot(np.log(range(len(dv_logcounts))),dv_logcounts,'r')\n",
      "plt.xlabel('log rank')\n",
      "plt.ylabel('log count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 273,
       "text": [
        "<matplotlib.text.Text at 0x20bb7d30>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEPCAYAAABLIROyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm8lnP+x/HXWdpOJS1aJE4LEW1Mg9a7pqgUUrKVpDBE\nmcaSmaGFH8YaQ8aMpY41ldKKopMWQ1KJCpUSKanEKeR07t8fn/s45+RU933d13Vf9/J+Ph7349z3\ncV/f871/0+/63N/t8wEREREREREREREREREREREREREREZESngG2AauK/a4aMBf4DHgTONKHfomI\nCJDuYdvPAl0P+N0ILACcALwVei0iIkkom5IjgLVArdDz2qHXIiLiAy9HAKWphU0LEfpZ6xDvFRER\nD8U6ABQXDD1ERMQHmTH+e9uwqZ+tQB3g29Le1LBhw+D69etj2S8RkWSwHmgU7ptjPQKYDgwIPR8A\nTCvtTevXrycYDCbtY+TIkb73QZ9Pn02fL/keQMNIbsheBoCXgCVAY2AzMBC4F+iCbQPtFHotIiI+\n8HIK6JKD/L6zh39TRETC5OcicMoKBAJ+d8FTyfz5kvmzgT5fqknzuwMHEQzNZ4mISJjS0tIggvu6\nRgAiIilKAUBEJEUpAIiIpCgFABGRFBXrk8Bhu/xyKF8++kflyvYQEZGS4jYAdO4MP/9c8pGXB999\n9/vfl/b46Sf7+cMPUKUKNGkCJ59c8me1an5/ShER/yT9NtBgEDZvhtWr4ZNP7Gfho0KFomBQGBha\nttSIQUQSU6TbQJM+ABz8D8DXX5cMDB9/DKtWQaNG0Lq1Pc48Exo0gLR4/b+UiEiIAkCU9u2D5cvh\n3XdhyRJYvBjy84sCwpVXQvXqvnRNROSQFABcVjiF9O678MYb8Oab8Oyz0KWL3z0TESlJAcBj8+bB\nwIFw4YVw992200hEJB4oFYTHOneGFSvgyy/h9NNt/UBEJBEpADhQvTpMmgTDhkEgAI8+alNFIiKJ\nRFNAUVq3Di67zHYJjRwJXbtqx5CI+ENrAD7Yvx+mTIE777Q1gdtvh549FQhEJLYUAHxUUADTpsGY\nMXbzHzoUzjkHatb0u2cikgoUAOJAMAgzZkBOju0aatwYevSwBeRjj7WAUKaM370UkWSjABBn9u2D\nhQth1ix45x3YsgW2b4eqVeGss+Dxxy1XkYhItBQAEsD+/RYE7rzTDpZNmgQtWvjdKxFJdAoACeal\nl2yt4J57YNAgLRyLiHPJEwA2boTMTMjIOPjP9PSkuGOuXWsni1u2hCeegIoV/e6RiCSi5AkA9erZ\nXEl+fsmfxZ8XFFggOFSQyMqyXM/Nmxc96te34BFH9u6F666DpUth4kQ45RS/eyQiiSZ5AkA4U0DB\n4O+DwoGBIi/P8jyvWAErV9rj+++hadOSQaFp07j46v3ss3DzzVaspn17e964sd+9EpFEkFoBwKmd\nO+Gjj4oCwsqVsGYN1KsHbdvafs1OnaBWLe/6cAgFBZZj6PHHrbLZhAm+dENEEowCgFO//mqT8QsW\n2Ob93FzbtN+5sz3at4dKlWLapQ0brAbBli1xN2MlInFIAcAt+fnwwQfw1lsWEJYutVXakSMtIMRI\n48a2JqBtoiJyOAoAXtm7F2bPthSg/frZJv6yZT3/s0OHwtFHw4gRnv8pEUlwqgfglaws6NPHFpNX\nr4Y2beDzzz3/s127wuuve/5nRCQFKQBE6qijYPp0GDDAJugnTPC0GECHDrBsGfzwg2d/QkRSlAKA\nE2lpcP31tj5w331w6aW2s8gDFSvCmWfC22970ryIpDAFgGg0a2YLxTVq2OGyPn0sH/Qvv7j6ZzQN\nJCJe0CKwW3btsqowzz9vB8/69LGqMC1bQp06UaWsWLMG2rWzdef+/WO+G1VEEoR2AcWDL7+EF1+0\neZvly20Tf4sW0KoV9O5tzyMMCAsWwCOPWErp8eOtvoCISHEKAPEmGLSTXMuXw6JF8MorVg3m4ott\nhNCsWUTbSd97D84/H+66y7KHiogUSpQAcBvQDygAVgEDgeIT58kTAA4UDNqhspdftgNm69bBySdb\ndZgxYyyB3WF89pmNAHbtghNPtEt79oTjj4+LdEYi4pNECADZwNvASdhNfyIwGyie8SZ5A8CB9uyx\nswV//zuceio89FBYlwWDsG2bLTfMmAFvvAGbNlnGimnToEIFj/stInEnEQJANeBd4AzgR2Aq8Agw\nr9h7UicAFNq1y84VXH89DBniqIn9++Hyy62padNiclBZROJIIpwE3gk8CHwJbAG+p+TNPzVVrWqF\ng//v/2DmTEdNZGTYAvFPP1laaRGRQ/FjBNAQmAG0A3YDk4DJwAvF3hMcOXLkby8CgQCBQCCGXfTR\ne+/ZhP6wYXDrrVbYJkLvvGMLxGvXhrWkICIJKjc3l9zc3N9ejx49GuJ8CugioAswOPS6PzYdVHze\nI/WmgIrbvBkGDrT1gYkTLS11BIJBm0266SbbdSoiqSERpoDWYjf8ClhHOwOrfehH/KpXD9580+7i\n//hHxJenpdll115rxeZ37/agjyKS8PwIACuBHOAD4KPQ7/7jQz/iW3o63HKLJZ7bsyfiy885B+bP\nt1PEDRtasXkRkeJ0ECzede8Ol11mD4c+/9zODbRta8GgQgVLZlqtmov9FBHfJcIUkESif3947rmo\nmjj+eFi4EI47Dn780dJK9O5tVTBFJHVpBBDv9u6FunVtLqd2bVea3L8fLrjAkpaOGmXZKLKyXGla\nRHykEUCyycqC886Dl15yrcmMDEtcetppcOWVcOGFrjUtIglEI4BE8NZbdi7ggw+gfHlXm963z1IR\n/etfVndARBKXRgDJqGNHaNrU0oD+/LOrTZctCw8+CMOHQ36+q02LSJzTCCBR5OfbgvCOHZYrKDvb\ngkJ69DE8GIQuXexnz562Q6hq1ei7LCKxpRFAssrMtN1ArVvDU09ZxbE//MFqRUYZLNPSLDv1VVdZ\nyYJjj4V+/Sy7qOKwSPLSCCBRBYPw6qt25LdmTasX2batKyOCvDyrdf/ww3D66Va2QETiXyKkgw6H\nAkC48vMhJwfuvx927rR80KNHu7KvMz/fyhl/8IGdIRCR+KYpoFSTmWl7OdesgXfftfKTLVvC11+7\n0vS558LQoVbRsqDAhf6KSNzQCCAZ3Xij/Rw7NuqmvvrKlhwef9wCwtixlp2icuWomxYRl2kKSGwU\ncMopVjy4Rg3Xmn33XRsNrF1rhcvuuce1pkXEBQoAYq6+GmrVssVhl+3caYvDY8da1lERiQ9aAxDz\nt7/Z3M2UKa43Xa2a1a7v1w8uvdRKUIpI4lEASFbZ2TBnDlx3naWScFnPnrBqlR1Mtip0IpJoNAWU\n7HJz4aKL4LHHLOlPkyauNr9lix1IXrwYTjzR1aZFJEKaApKSAgHL9PbMM/CnP1m1+OnTLRe0C44+\n2qaDWrWyfEKK2yKJQyOAVLJ7t63czp1rlWH++U9LApSREXXTu3ZBhw5w882WskhEYk+7gOTwgkGr\nL/DAA1YW7KGHLBBEacUKOOssmDDBmsvMdKGvIhI2BQAJXzAI06bZ1/YLL3RlY/9zz8Ejj1jS0tGj\nbaeQC+mJRCQMCgASuR077ODY1KlwxhmuNPnaa3D33VCuHAwcaDXty5Z1pWkROQgFAHHm1VdtFXfJ\nElvZdcH+/TYdlJMDFSrYrNORR7rStIiUQruAxJkLLoA//9m2iV58sSWXi1JGhuWpe+MNq2d/1lm2\nDi0i8UEBQIqMGAEbNlihmXbtrELMr79G3Wy5crYLtVUr6NQJ1q1zoa8iEjUFACmpWjW46SbL+LZx\nI9xxhyub+9PS7DhCv37Qvr3NNImIv7QGIAf3zTfQrRv06AF33eVas6++anWHr7zS6tecdpprTYuk\nNK0BiHvq1LF8QuPGwcyZsG2bK81ecAEsXQpHHAFdu9r5ARGJPY0A5PBycmD8eLtTV61qmUYHDXKl\n6fvus4L0/ftbnft69VxpViQlaRuoeCcYtK/uF14Iw4bBDTdAmTJRNZmfDy++aDuFPv7YYkxavP6r\nFIlzCgDivY8+srKT27bBf/4DbdpE3WR+vmUV7djRZpxEJHJaAxDvNWsG8+ZZEBg0yJUzA5mZllJ6\n6lQ7MCYi3lMAEGfS02HwYCs92batfXXfvDmqJqtVszWBMWOs/rCIeEsBQJxLS7P0EZ9/bhXInn02\n6iYvvdQOJJ97LixfHn0XReTgtAYg7liyBPr2tQpkjRpF3dyECTBkiKWPmDJFC8Mi4dAagPijdWu4\n9VY7OLZ+fdTNDRhgSUo/+QSeftqF/onI7/gVAI4EJgNrgNWAOzmIxV833GBTQs2aQf36dhdftsxx\nc+XKwfPP25rA9de7kpZIRIrxKwA8AswGTgKaYYFAksG111p9yDlzLLNo166WCa6gwFFzrVrZ7NLC\nhVbNUkTc48fMahVgOdDgEO/RGkCyWLzYtou2a2elJx369FNrorDGfa1a7nVRJFkkwkGwFsCT2NRP\nc2AZMAzYW+w9CgDJZMcOOOEEWLDAKo85lJdnA4xt2+D111VqUuRAibAInAmcCowL/dwDjPChHxIr\n1avDgw/adNBrrzlOL12pEjz6qNUTcKF8sUjKywzjPW2BRQf8rg2w2OHf/Cr0WBp6PZlSAsCoUaN+\nex4IBAgEAg7/nMSFK66wRHK33mq7hIYPd9RM1aqWTrpDBzjnHGjRwt1uiiSS3NxccnNzHV8fzlBh\nOdAyjN9F4h1gMPAZMAqoANxa7L9rCihZbdgALVtaCon777e6kQ6MHm2DidmzrdykiEQ+BXSoEcCZ\nQGvgKGB4sUYrE/3U0Q3AC0BZYD0wMMr2JFE0aGAruh06QFYW/OMfUL58xM387W9WsKxBAwsGN92k\nw2IikTrUjbwsdrPPCP2sFHr8APSJ8u+uBFphi8AXACoVnkpq14bnnrO9nfXr25xOhMqUscwTa9bA\nCy/AX/8KP/zgQV9Fklg435mygY3eduN3NAWUCgoKYMYMS/7z1FM2qe/A1q12Bu3tty07de/eLvdT\nJEF4sQ20MXATFggKp4yCQKcI+xYJBYBUMmuWlQRr186O/TZv7qiZDz6wpKRr1sAxx7jcR5EE4EUA\n+Ah4AvgQ2B/6XRDbv+8VBYBUs22bZYC75x6bEurY0VEzQ4daPrr58233qUgq8SIALANOc9ohhxQA\nUtWcOZZDaNo0SzAXoWDQNhht2mRlJjPD2egskiS8OAg2AxgC1AGqFXuIuK9bNytA36EDTJoU8eVp\nafDEE7B/v2t160WSVjiRYiM25XOg+u52pQSNAFLdK6/ARRfBd985msvZuxdOPhl69rSdpjVretBH\nkTjjxQggG7vZH/gQ8U7fvpb4JzvbTnxFKCsLFi2yYvONGsEdd7jfRZFEF06kGEDpI4Acl/tSnEYA\nYhYutPqQ06fbLiEHvv7a0kpffTWMGOHo3JlIQvBiBNCq2KM9lrrhXAd9E4lcu3Zw992WS+j99x01\nUbeu7QpascJODqvgvIhxcnj+SGAicLbLfSlOIwApsn8/3HknvPgizJsHxx7ruKmXXrI1gXXrlDpC\nkk8s0kHvRWsAEksZGTByJFxyidUTmD7dcVMXX2xryn36OC5SJpI0wokUM4o9TweaAK9QMnun2zQC\nkNItWgTnn2+nh08/3VETP/5olcWaNLGC82XLuttFEb94cRAsEPoZBPKBL4HNkXYsQgoAcnAPPwyj\nRsHUqdDJWUaS7dvhssvs4Njs2ZZcTiTReVUSsja2CBwE3ge+jbhnkVEAkIMLBmHiRMsA98wzttnf\ngfx86NULfv7ZdppmZbncT5EY82INoC/wHnBh6Pn7oeci/khLs8n8qVMtbcRuZ9nEMzNhyhRbEwgE\n4OOP3e2mSLwLNxlcZ4q+9R8FvAU086pTaAQg4briCqsW//TTUKWKoyZ+/RXuvdcKlN1+OwwbpnUB\nSUxeTAGtwm72hXfkdKygS9NIOxcBBQAJz+7ddlDshBOsGEAUeztXrYLBg+GII7QuIInJiymg14E3\ngCuw0o2zgTkO+ibivipVbHP/0qVw6aW2uutQ06aweLFtDx0yxMU+isSpcCNFb6BN6PlCYKo33fmN\nRgASmZ9+gr/8xdJJv/46nHSS46Z27YI//hG6doWxYx3XrReJOS+mgOoDW4GfQq8rALXwtkykAoA4\nk5MDt9xi23ocnhMAKzPZq5cNKN5801JIiMQ7L6aAJlNUCQygIPQ7kfhz+eUwbhyccQY89phtGXWg\ndm1YssQ2GzVqBHPnutxPkTgQTgDIAPYVe/0LoOUxiV8XXGCT+ePG2fNNmxw1k5YGd91ldeu7doWZ\nM13up4jPwgkA3wHnFXt9Xuh3IvGrdWurEl+9umUU3bDBcVPnnGNB4PzzYflyF/so4rNwAsCfgb9h\n6R82AyOAa7zslIgrsrLgqafgqqusPNg99zhuqnt3y0rdqhX8+98u9lHER5Fsmq4c+vmjFx05gBaB\nxV2ffAJduthoYOJEx83Mnw9nnw3XXw8PPeRi/0Rc4FUuoFhTABD3/fwzHH+8JZPr08dxM99+ayUJ\neveGZ5/VqWGJH7GoByCSmMqXh5dfhoEDo6opULMmfPEFrF1rO02//trFPorEkAKApJY2bezmf955\nNqnvUJ06ttGoQQPbcfqt1/lxRTwQzlChN78vCr8byxHk1T97TQGJtzZtstwPr70GHTs6biYYtAHF\n1KkwebItM4j4xYs1gFnAmcD80OsA8CF2QngMkBNRD8OjACDemzrVsolOnGgru1Ekkhs/Hq6+Gs46\ny2KK0keIH7wIAG8C/YFtode1gOeAS4B3gJMj62JYFAAkNp5/HoYOhR49YMKEqIJAYQ6hmjUtCNSo\n4WI/RcLgxSJwPYpu/mDTPvWAHZQ8ISySePr1sxXdd96BP/8Z9jn/J121atHZs6OPtjUCkXgWTgCY\nj00DDcBSQk8HcoGKwPdedUwkZqpUgXnz7O7dtKnjCmOFTU2fbikk2raF++5znI5IxHPhDBXSgQso\nSge9GJjC7xeG3aQpIIm9YNBSgC5aZFlFu3ePqrlZs+DCC22ReOxYFZgR73ldFB6sPrCKwktyCgbh\n0UfhtttsYfjll6FcOcfNrVxpieSaNbMyBVEsMYgclhcBoC9wP7Ag9Lo9cDMwKdLORUABQPz17bfQ\nrZsVBpg0yZLLOfTjj3ZeoEEDWLhQJ4fFO14sAv8D+/Z/eejRCrjdSecOkAEsB2a40JaIu2rWtDWB\nQYPs8Ni4cY6bqlzZkpHm5Vkyubw8F/spEoVwAkAaULzQ6g7cySE0DFiNt2sJIs6lpcGYMTZ3c8MN\n0L+/4xXdypXh/fft8lNOgc8+c7mvIg74VRT+GKA78BTxm5BOxJx9Nnz+udWGPO002LHDUTMVK8Ky\nZdC+PTRubJmqRfwUTgC4BXgSaA40DT2/Jcq/+zC2jlAQZTsisdGggQWBGjWsXuTHHztqpkwZ22D0\nxhtWpuCyyyxJqYgf/Pj23QPoBgzB0kr8Feh5wHu0CCzxKRiE4cNtX+dHH9m5AYfWrbPUEfn51tSR\nR7rYT0lJkS4CZx7iv+Vx8Pn5IHBE+N0qoTVwLjYFVD7UTg62wPybUaNG/fY8EAgQCAQc/jkRF6Wl\nWT2BMmVsb+dTT9lCsQONGtlaQKdOll1061Y7SCYSrtzcXHJzcx1f7/f8ewfgJjQCkET0+ONWGmzy\nZKsO49D+/dCwIRxzDOTmQuahvpaJHIKbI4BY0Z1eEtOQIbB9u1UXW7YMTj3VUTMZGXbjr18fzjwT\n/vc/ZROV2PB7BHAwGgFIYigosEyir78OGzdarUiHvvnGksiddpoFAY0EJFIqCSkSS+npMGOGnRru\n0gV++MFxU3XqwPr1sGqVzSjt3etiP0VKoRGAiBvy821H0BdfwOrVtm3UoZUroUULOO44+OQTOz8g\nEg6NAET8kJlpZwM6dbIV3WuusekhB5o3t4zUe/bYkYNt2w5/jYgTGgGIuG3BAggErDzYrFmOS4P9\n9JPlDvrkE/jqK6hb191uSvLRCEDEbx06WNH5zZuhSRPbJpqfH3EzFSrY5qJmzayZlSs96KukNAUA\nES8ce6x9db/kEqsK07q1lZ2MULlyVp+mXTtbF5jkZRJ2STkKACJeqVoVHnnEJvRPPhl69rREQBGq\nXBlmzrTEpH37qtawuEcBQMRrRxwBzzwD//ynZYAbOdKO/0bo9tttbTkQsOcOZpVEStAisEgszZgB\nl18O550H48c7amLOHKsz3L49PPmkDTREwLuawLGmACDJq3BS/5ZbYPRoKF8+4ibefx+uvdZ2Ci1Y\nAEcd5UE/JeFoF5BIvGvbFubPh//+F/7wB/j004ib+OMf4e237cZft67jwYSkOAUAET8EArBliyX/\nOeUU2zYaoSpV7Nv/o4/alJAqjEmkNAUk4qf8fGjZ0qrBTJjgOIXEfffBrbdaqYIbb3S5j5IwNAUk\nkkgyM21hODMTrrjCcTK5m2+Gf//bgsDw4e52UZKXRgAi8WDjRuje3aaEcnLspwOzZ8PFF8PQoXDX\nXe52UeKfRgAiiSg724755uVZgZmZMx010707TJsGDz5o00Eih6IRgEg8WbcOHnvMksh17gzjxlkd\n4giNHw9//Sv06gVPPGEljCX5JWJJSBEp1KiRnQ044wzb6J+WZhP89etH1Ez//pZCYsgQCAZtl1Db\nth71WRKWRgAi8eq11+D++216qFcvR4XnZ82C556zcpM33mjNSPLSSWCRZLJgAUycaJv8Fy60lKDl\nykXUxPr1tiYwfrw10bKlVbKU5KMAIJKMeve2dNIPPwz9+jlqomdPyM2Ft96yk8SSfLQLSCQZTZli\nqUCvusqyijowYwaccw507Ahjx7rcP0lIWgQWSRRjxlil+AcftILBY8ZE3MRzz9muoCeftEXiQYM8\n6KckDE0BiSSSnTttTeDGG+HDDy11RIUKETWxfTv861/wyit23KBBA60JJAutAYikgh494L33YMAA\neOCBiC///nvbFrpxI7z8sjUniU8BQCRV5ORYvodOnezsQK1aETdx5ZW2S+iMM+Deex2dOZM4okVg\nkVTRo4cVlVmyxKaDHLjpJttU9MgjsHevy/2TuBev8V4jAJFwXX21jQaysiwQZGdH3ETjxvDVV3Dq\nqXZWQBKTRgAiqeaJJ2xl94QTrNzk559b/ocIrFoFq1cX/dyyxaO+SlzRCEAkWYwYAdOnw4YNNhJo\n0iSiy/PzrVTxzp1WluCbbzzqp3hGi8Aiqa5dO7j0Uksb0bJlxEXnf/nFzggsWAC1a0ech058pCkg\nkVR39tm2JnD++XZmIELlykHXrpaM9LzzPOifxA2NAESS1bBhUL26LRIfeWTEI4FNm6B1a1i2DDIy\n4KijPOqnuEYjABExTZpYQZmTTrJ6wxGqUcNiRosWULeuBQJJLgoAIsnqmmtg61YrNbl9e8SXV6xo\nh8S2boVAAHbscL+L4i8FAJFkV6UKfPCBnRju1MkKA0ToiCOsxGSnTvD00+53UfzhxxpAPSAHqAkE\ngf8Ajx7wHq0BiLhl/35YvNj2eb75pm3yz8mJqInNm+14wVtvWf6gF17wpqsSnUSoCfwr8BdgBVAJ\nWAbMBdb40BeR5JeRAe3b2/Ndu6y62ObN9rpOHcg8/G2gXj17/PADLF1adHndusokmsj8+J9uK3bz\nB8jDbvxH+9APkdTToIEd9W3dGk4+2ZIARaB+ffj0U7v8xBMtk6gkLr8LwmQDLYH3fO6HSGpo2dL2\nd4JlEt21K6LLmzcvunzIkIgvlzjjZwCoBEwGhmEjgRJGjRr12/NAIEAgEIhVv0RSQ1aWpY3YsMFe\nV6hgU0IRXL55c9HltWrZziGJndzcXHJzcx1f79dBsDLATGAOUFp1Ui0Ci3ht9my4/vqi15s321f6\nSpXCunzCBCtDAJCXZ9mpn3nGg35K2BIhF1AaMAHYgS0Gl0YBQCTWatWCjz5yVFhm8mRbD5g82YN+\nSdgS4SRwG6Af0BFYHnp09aEfIlJc+fLw00+xvlR8pFxAImKaNLE1gMIpoLp1LZVEGObPh759bXcQ\nWGnJu+6CU07xqK9SqkSYAgqHAoBIrC1fDl9+ac/37YP+/eHnn8O69JdfYO5cO3MGMHas1Rvu39+j\nvkqpFABEJHoFBXaArKDAUaX4wYOt0PzgwR70TQ4qEdYARCTepadDmTI2EnCgXLmwBw/iI78PgolI\nvCpfHgYNskBQaPhwaNo0rEtzckqmkB48GNq08aCf4pgCgIiUbuJEywVdKCfHksqFEQCuu67kAvC0\naZZITgEgvigAiEjpunUr+Xr58rCnhBo2tEehr7/WlFA80hqAiISnbFnHawJRXCoeUgAQkfBEsSgc\nxaXiIU0BiUh4ypWzfA+rV5f8fXa2nfo6zKWzZ8N335X8/TXXQLt27nZTwqdzACISnq++ggMzT+bl\nwd//ftiCwdu3wxtvlPzd5Mlw6qlwxx3udjOV6SCYiMTO7t1FpcIiNHq0nRweM8aDfqUoHQQTkdjJ\nzLRaww6UKQO//upyfyQiCgAi4lwUd/EyZRzHDnGJAoCIOFc4AnAwZZuZqRGA37QLSEScS0+3x6JF\ndkc/ULVq0LhxqZeWKWNFyN599/f/7eij4bjjXO6r/I4WgUUkOr16lUwZUSg/H9avh507S70sNxdu\nu+33v9+zx3IJvf++u91MBdoFJCLxIS/Pykvu2RPRZStWwIABsHKlR/1KYtoFJCLxISOjqEJMBDIz\nHV0mDigAiIg3HAYAh5eJAwoAIuKNKAKAtofGhgKAiHgjPd22hxYURHSZRgCxowAgIt5IS3N0N1cA\niB0FABHxjsMAoCmg2NBBMBHxTkYGrFoFFSoc/n2NG0N6OhkZ8Msv8PHHh76kYkWoX9+9rqYinQMQ\nEe9062bHfQ/niy8sX3TbtuzdC4EA7N176EvWrrWjBuXLu9LTpBDpOQCNAETEO3PmhPe+P/3JvvYD\nWVnhnQKuVElTRdHSGoCI+C89PeLdQg4ukQMoAIiI/9LSIr6bO7hEDqAAICL+Kzwz4O0lcgAFABHx\nn6aAfKEzsAe2AAAFUElEQVQAICL+UwDwhQKAiPhPAcAXCgAi4j8FAF8oAIiI/xQAfKEAICL+UwDw\nhV8BoCuwFvgcuNWnPohIvEhLi3hPp4NL5AB+BIAM4DEsCDQBLgFO8qEfvsnNzfW7C55K5s+XzJ8N\nfPx8MRoBJPv/fpHyIwD8EVgHbAR+BV4GzvOhH75J9n+Eyfz5kvmzgQJAqvEjANQFiqcH/Cr0OxFJ\nVVoD8IUf2UA1ayciJWVmwgMPwMsvh33Jf7fCt2fAzoySv//PKY+yLav0QgGffgrLltnz8uVh0iSn\nHU4OftQDOAMYha0BANwGFAD/LPaedUDD2HZLRCThrQca+d2JQ8nEOpkNlAVWkGKLwCIiqawb8Cn2\nTf82n/siIiIiIiJ+SuZDYvWA+cAnwMfAUH+744kMYDkww++OeOBIYDKwBliNrWclk9uwf5urgBeB\ncv52J2rPANuwz1OoGjAX+Ax4E/vfNFGV9vnux/59rgReBar40C/HMrBpoWygDMm3PlAbaBF6Xgmb\nBkumzwcwHHgBmO53RzwwAbgy9DyTBPt/rsPIBjZQdNOfCAzwrTfuaAe0pOQN8j7gltDzW4F7Y90p\nF5X2+bpQtL3/XhLs850JvF7s9YjQI1lNA/7kdydcdAwwD+hI8o0AqmA3yGRVDftCUhULbjOAzr72\nyB3ZlLxBrgVqhZ7XDr1OZNmU/HzF9QKeP9TF8ZYMLpUOiWVj0fs9n/vhpoeBm7FtvcmmPrAdeBb4\nEPgvkOVrj9y1E3gQ+BLYAnyPBfNkUwubNiH0s9Yh3pvorgRmH+oN8RYAUuWQWCVsLnkYkOdzX9zS\nA/gWm//343yJ1zKBU4FxoZ97SK7RaUPgRuyLydHYv9HL/OxQDARJ3nvO34F92FrOQcVbAPgaWygt\nVA8bBSSTMsAUbGg2zee+uKk1cC7wBfAS0AnI8bVH7voq9Fgaej0ZCwTJ4g/AEmAHkI8tILb2tUfe\n2IZN/QDUwb60JJsrgO4kYABP9kNiadhN8WG/O+KxDiTfGgDAO8AJoeejKHl6PdE1x3amVcD+nU4A\nhvjaI3dk8/tF4MLdhSNIsEXSUmRT8vN1xXZy1fClNy5I5kNibbH58RXYVMlyilJiJJMOJOcuoObY\nCCAht9iF4RaKtoFOwEariewlbD1jH7a2OBBb7J5HcmwDPfDzXYltn99E0f1lnG+9ExERERERERER\nEREREREREREREREREXEuXtJtZHPwxF0iMRdvqSBEvOBFvpeMw79FJL4pAEgqScMKZqwCPgL6hn6f\njp2YXIOdDp0F9C7l+lwsjcdSLJFfD+B/WHbQuUDN0PtGYcU65mOpTW4opa0GoetOi+oTiYjIIf0Y\n+tkbu8GnYTfrTVhisD7YTR8sPfBO4IJS2pkPPFbsdfE0AoOBB0LPRwGLsFQK1YHvsBFDNhZ8GmM3\n/6aOP5GICzL97oBIDLXF0uMGsSyQC4BWQBvgldB7tmE3+oOZWOx5vdB1tbHkhYUFY4JYQPkVy675\nLUV552tiWWB7kfjFSCTBaQpIUkmQg9cqCLeGwZ5iz/8FPAo0A67BMmkW2lfs+X6Kvmx9j4082oX5\n90Q8owAgqWQhcBH27/4ooD1WkW0xNj2Uhn1TDxyijeKB4ggsGyNYDvbS3nOgfdj00uXAJWH3XMQD\nmgKSVFC4C2gqVnd6Zeh3N2PTM1Ow2syrsbS6HwK7D9MW2Fz/JGAX8DZwXLH3HGznURDYiy0gz8XW\nJ2ZG+HlERMRFFUM/q2O1KGoe4r0iSUEjABEzE9vVUxYYQ3KWChQRERERERERERERERERERERERER\nEZHE9/8g9jPJQzd64QAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x20bad0f0>"
       ]
      }
     ],
     "prompt_number": 273
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1**\n",
      "\n",
      "- Explain what you see in the plot. Does it observe Zipf's law? How well?\n",
      "- Print the token/type ratio for both the training and dev data.\n",
      "- Print the number of types which appear exactly once in the training and dev data\n",
      "- Print the number of types that appear in the dev data but not the training data (hint: use [sets](https://docs.python.org/2/library/sets.html) for this)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The plot observes Zipf's law in the middle region. However, it fails to obey that in extreme areas (having very high and very frequency regions).\n",
      "print 'test'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this enables you to create inline plots in the notebook \n",
      "train_count = 0.0\n",
      "train_unique_count  = 0.0\n",
      "dev_unique_count  = 0.0\n",
      "dev_words = []\n",
      "     \n",
      "for key in ac_train.keys():\n",
      "    train_count =train_count+ac_train.get(key)\n",
      "    if(ac_train.get(key) ==1):\n",
      "        train_unique_count= train_unique_count+1\n",
      "   \n",
      "print \"number of train word tokens\", train_count\n",
      "print \"number of  dev word types\",len(ac_dev.keys())-1\n",
      "dev_count = 0.0\n",
      "\n",
      "for key in ac_dev.keys():\n",
      "    dev_count =dev_count+ac_dev.get(key)\n",
      "    if(ac_dev.get(key) ==1):\n",
      "        dev_unique_count= dev_unique_count+1\n",
      "    if(ac_train.get(key)==None):\n",
      "         dev_words.append(key)\n",
      "   \n",
      "print \"number of dev word tokens\", dev_count\n",
      "\n",
      "print \"Train ratio\", train_count/(len(ac_train.keys())-1)\n",
      "print \"Dev ratio\", dev_count/(len(ac_dev.keys())-1)\n",
      "print \"Dev single Count words\",dev_unique_count\n",
      "print \"Test single count words\",train_unique_count\n",
      "print \"Words unique in dev \" , len(dev_words)\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of train word tokens 466337.0\n",
        "number of  dev word types 16900\n",
        "number of dev word tokens 223858.0\n",
        "Train ratio 18.7577732191\n",
        "Dev ratio 13.246035503\n",
        "Dev single Count words 8012.0\n",
        "Test single count words 10807.0\n",
        "Words unique in dev  4796\n"
       ]
      }
     ],
     "prompt_number": 274
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. Word Lists #\n",
      "(_Completing predict() - 3 pts, setting weights - 2 pts, Deliverable 2 - 1 pt. Total 7 pts for part 2_)\n",
      "\n",
      "- We will now build a sentiment analysis system based on word lists. \n",
      "- The file \"sentiment-vocab.tff\" contains a sentiment lexicon from [ Wilson et al 2005](http://people.cs.pitt.edu/~wiebe/pubs/papers/emnlp05polarity.pdf). \n",
      "- The code below reads the lexicon into memory, building sets of positive and negative words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poswords = set()\n",
      "negwords = set()\n",
      "with open('sentiment-vocab.tff','r') as fin:\n",
      "    for i,line in enumerate(fin):\n",
      "        # more list and dict comprehensions!\n",
      "        kvs = {key:val for key,val in [kvp.split('=') for kvp in line.split() if '=' in kvp]}\n",
      "        if kvs['type'] == 'strongsubj':\n",
      "            if kvs['priorpolarity'] == 'negative':\n",
      "                negwords.add(kvs['word1'])\n",
      "            if kvs['priorpolarity'] == 'positive':\n",
      "                poswords.add(kvs['word1'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 275
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, you should write a classifier that classifies each instance in a testfile. The classification rule is:\n",
      "\n",
      "- 'POS' if the instance has more words from the positive list than the negative list\n",
      "- 'NEG' if the instance has more words from the negative list than the positive list\n",
      "- 'NEU' (neutral) if the instance has the same number of words from each list\n",
      "\n",
      "To do this, you will write a function \"predict\", \n",
      "which represents the inner-product computation $\\boldsymbol{\\theta}' \\boldsymbol{f}(\\boldsymbol{x},y)$.\n",
      "It should have the following characteristics:\n",
      "\n",
      "- **Input 1** an instance, represented as a dict (with features as keys and counts as values) \n",
      "- **Input 2** a dictionary of weights, with labels as keys. Each value is itself a dictionary, with features as keys and weights as values. This corresponds to $\\boldsymbol{\\theta}$ in the reading. See example below.\n",
      "- **Input 3** a list of possible labels\n",
      "- **Output 1** the highest-scoring label\n",
      "- **Output 2** a dict with labels as keys and scores as values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use this to find the highest-scoring label\n",
      "#print poswords\n",
      "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 276
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(instance,weights,labels):\n",
      "  #  scores = # your code here\n",
      "    # return the highest-scoring label, and the scores for all labels\n",
      " #   print instance\n",
      " #   print weights\n",
      "  #  for word in instance:\n",
      "  #      print word\n",
      "   classifer = Counter()\n",
      "   for word in instance:\n",
      "       for label in labels:\n",
      "         #   if weights[label,offset] != 0:\n",
      "          #      classifer[label] +=  weights[label,offset]*instance[word]\n",
      "           #     instance\n",
      "           # else:\n",
      "                    \n",
      "         #       print label\n",
      "         ##       print word\n",
      "          ##      print weights[label,word]\n",
      "           ##     print instance[word]\n",
      "            #    print classifer[label]\n",
      "           \n",
      "            classifer[label] +=  weights[label,word]*instance[word]\n",
      "            #    print classifer[label]\n",
      "\n",
      " \n",
      " #  if classifer[argmax(classifer)] == 0 :\n",
      "#        return 'NEU', classifer\n",
      "   return argmax(classifer), classifer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 277
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are weights for the simplest classifier, which simply labels all instances as positive.\n",
      "\n",
      "Note that it uses only the 'offset' feature"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_labels = ['POS','NEG','NEU']\n",
      "weights_all_pos = defaultdict(int)\n",
      "weights_all_pos.update({('POS',offset):1,('NEG',offset):0,('NEU',offset):0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 278
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is some code for evaluating your classifiers. \n",
      "It uses a scoring library that I wrote, and writes the output to a file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evalClassifier(weights,outfilename,testfile=devkey):    \n",
      "    with open(outfilename,'w') as outfile: #open the output file\n",
      "        for counts,label in dataIterator(testfile): #iterate through eval set\n",
      "            print >>outfile, predict(counts,weights,all_labels)[0] #print prediction to file\n",
      "    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 279
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below shows how to evaluate this classifier. \n",
      "\n",
      "- **Sanity check**: You should get 40.7% accuracy just by classifying everything as positive. This is the \"most common class\" (MCC) baseline.\n",
      "\n",
      "The printed output is a **confusion matrix**. \n",
      "The rows indicate the key and the columns indicate the response. \n",
      "In this case, the response is always \"POS\", so there is only one column. \n",
      "The cell NEG/POS tells you how often an example that was labeled \"NEG\" in the key was labeled \"POS\" in the system response."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mat = evalClassifier(weights_all_pos,'all_pos.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
        "1 classes in response: set(['POS'])\n",
        "confusion matrix\n",
        "key\\response:\tPOS\n",
        "NEG\t\t401\t\n",
        "NEU\t\t192\t\n",
        "POS\t\t407\t\n",
        "----------------\n",
        "accuracy: 0.4070 = 407/1000\n",
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 280
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now build a classifier based on the word lists. The classifier should have the following decision rule:\n",
      "\n",
      "- If the number of positive words (tokens) is greater than the number of negative words, choose label 'POS'\n",
      "- If the number of negative words (tokens) is greater than the number of positive words, choose label 'NEG'\n",
      "- If they are equal, choose label 'NEU'\n",
      "\n",
      "You should manually set the weights to ensure this behavior; don't change the predict function at all.\n",
      "You'll need to use the offset weights to make sure that ties go to the 'NEU' label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights_all_words = defaultdict(int)\n",
      "for posword in poswords:\n",
      "    weights_all_words['POS',posword]=1\n",
      "    weights_all_words['NEU',posword]=.5001\n",
      "for negword in negwords:\n",
      "    weights_all_words['NEG',negword]=1\n",
      "    weights_all_words['NEU',negword]=.5001"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 281
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2**: run your classifier on dev.key, and use the following code to print the resulting confusion matrix.\n",
      "\n",
      "The confusion matrix should now have three columns, since the response should include every class at least once. The count of correct responses is found on the diagonal of the confusion matrix. What is the most frequent type of error?\n",
      "\n",
      "**Sanity check**: The accuracy should be 55.9%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mat = evalClassifier(weights_all_words,'word_list.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
        "3 classes in response: set(['NEG', 'NEU', 'POS'])\n",
        "confusion matrix\n",
        "key\\response:\tNEG\tNEU\tPOS\n",
        "NEG\t\t204\t25\t172\t\n",
        "NEU\t\t58\t9\t125\t\n",
        "POS\t\t40\t19\t348\t\n",
        "----------------\n",
        "accuracy: 0.5610 = 561/1000\n",
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 282
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(include your explanation of the most frequent type of error here)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3. Naive Bayes #\n",
      "(_Completing learnNBWeights() - 5 pts, Deliverable 3a - 1pt, 3b - 1 pt, explanation of plot output - 2pts. Total 8 points for part 3_)\n",
      "\n",
      "Now you will implement a Naive Bayes classifier.\n",
      "\n",
      "You already have the code for the decision function, \"predict\". \n",
      "So you just need to construct a set of weights that correspond to the classifier. \n",
      "These weights will contain two parameters:\n",
      "\n",
      "- $\\log \\mu$ for the offset, which parametrizes the prior $\\log P(y)$\n",
      "- $\\log \\phi$ for the word counts, which parametrizes the likelihood $\\log P(x | y)$\n",
      "\n",
      "You should use maximum *a posteriori* estimation of\n",
      "the parameter $\\phi$,\n",
      "$$\\phi_{j,n} = P(w = n | y = j) = \\frac{\\sum_{i: y_i = j} x_{i,n} + \\alpha}{\\sum_{i:y_i=j} \\sum_{n'} x_{i,n'} + V\\alpha}$$\n",
      "where \n",
      "\n",
      "- $y_i = j$ indicates the class label $j$ for instance $i$\n",
      "- $w=n$ indicates word $n$\n",
      "- $\\alpha$ is the smoothing parameter\n",
      "- $V$ is the total number of words\n",
      "\n",
      "For each class, normalize by the sum of counts of words **in that class**. In other words, $\\sum_j \\phi_{j,n} = 1$ for all $j$. You can estimate $\\log \\phi$ directly if you prefer.\n",
      "\n",
      "For the prior $\\log P(y)$, you can use relative frequency estimation.\n",
      "\n",
      "Both probabilities should be estimated from the training data only.\n",
      "Please write this code yourself -- do not use other libraries, and try to do\n",
      "it without looking at other code online."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain #hint, especially if you're obsessive about being pythonic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 283
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def wordCount(line,fcounts,category,class_wordcounts):\n",
      "    sent_tokenize_list =sent_tokenize(line);\n",
      "    for sentence in sent_tokenize_list:\n",
      "            words = word_tokenize(sentence)\n",
      "            for word in words:\n",
      "       #             print word.lower()\n",
      "                    if word.isalpha():\n",
      "                        class_wordcounts[category]+=1\n",
      "                        fcounts[word.lower()][category] += 1\n",
      "# compute the word counts first, because this can be slow\n",
      "# you may also wish to keep a list of all word types that are observed in the training data\n",
      "counts = defaultdict(lambda : Counter()) # hint\n",
      "class_counts = defaultdict(int) # hint\n",
      "class_wordcounts = defaultdict(int)\n",
      "\n",
      "print total\n",
      "with open('train-imdb.key','r') as fin:\n",
      "    for i,keyline in enumerate(fin):\n",
      "        kvp = keyline.split();\n",
      "        class_counts[kvp[1]]+=1\n",
      "        dataloc = keyline.rstrip().split(' ')[0]\n",
      "        with open(dataloc,'r') as infile:\n",
      "             for line in infile:\n",
      "                        wordCount(line,counts,kvp[1],class_wordcounts)\n",
      "\n",
      "print class_counts['NEU']\n",
      "total = class_counts['POS']+class_counts['NEG']+class_counts['NEU']\n",
      "total = total *1.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2000.0\n",
        "404"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 284
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should write a *function* to compute the weights for a given value of $\\alpha$, \n",
      "because you will want to vary this value later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learnNBWeights(alpha=0.1):\n",
      "    weights = defaultdict(int)    # your code here\n",
      "    vocab =  len(counts.keys());\n",
      "    for word in counts.keys():\n",
      "        for label in all_labels:\n",
      "            weights[label,word] =log((float) ((counts[word][label] + alpha)/(float)(class_wordcounts[label] + vocab*alpha)))\n",
      "            #+ (log(class_counts[label]/total))\n",
      "    for label in all_labels: \n",
      "        weights[label,offset] = log(class_counts[label]/total) \n",
      "    return weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 285
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3a**\n",
      "Train a classifier from the training data, and apply it to\n",
      "the development data, with $\\alpha = 0.1$. Report the confusion matrix and the accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the weights\n",
      "weights_nb = learnNBWeights(alpha=0.1)\n",
      "#print weights_nb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 286
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: For the following instance, I get the scores shown."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print predict({'good':1,'worst':4,offset:1},weights_nb,all_labels)\n",
      "            #+ (log(class_counts[label]/total))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('NEG', Counter({'NEG': -34.749353528232476, 'NEU': -40.660711213141248, 'POS': -44.299017281808069}))\n"
       ]
      }
     ],
     "prompt_number": 287
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this code to evaluate your weights\n",
      "mat = evalClassifier(weights_nb,'nb.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
        "3 classes in response: set(['NEG', 'NEU', 'POS'])\n",
        "confusion matrix\n",
        "key\\response:\tNEG\tNEU\tPOS\n",
        "NEG\t\t328\t29\t44\t\n",
        "NEU\t\t90\t37\t65\t\n",
        "POS\t\t77\t43\t287\t\n",
        "----------------\n",
        "accuracy: 0.6520 = 652/1000\n",
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 288
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3b** Try at least seven different values of $\\alpha$. Plot the accuracy on both the dev and training sets for each value, using [subplot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot) to show two plots in the same cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr_accs = []\n",
      "dv_accs = []\n",
      "alphas = [.003,.01,.03,.1,0.3,1.0,3.0,10.0,30.0,100.0]# your choice\n",
      "weights_nb_alphas = dict()\n",
      "for alpha in alphas:\n",
      "    print alpha,\n",
      "    # learn the weights\n",
      "    weights_nb_alphas[alpha] = learnNBWeights(alpha) \n",
      "    # evaluate on training data\n",
      "    confusion = evalClassifier(weights_nb_alphas[alpha],'nb.alpha.tr.txt',trainkey)\n",
      "    tr_accs.append(scorer.accuracy(confusion))\n",
      "    # evaluate on dev data\n",
      "    confusion = evalClassifier(weights_nb_alphas[alpha],'nb.alpha.dv.txt',devkey)\n",
      "    dv_accs.append(scorer.accuracy(confusion))\n",
      "    print dv_accs[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.003 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.623\n",
        "0.01 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.634\n",
        "0.03 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.646\n",
        "0.1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.652\n",
        "0.3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.665\n",
        "1.0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.679\n",
        "3.0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.685\n",
        "10.0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.68\n",
        "30.0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.647\n",
        "100.0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.584\n"
       ]
      }
     ],
     "prompt_number": 289
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this code to plot the accuracies\n",
      "subplot(1,2,1)\n",
      "plot(log(alphas),tr_accs,'bx-')\n",
      "ylabel('training accuracy')\n",
      "subplot(1,2,2)\n",
      "plot(log(alphas),dv_accs,'rx-')\n",
      "ylabel('dev. accuracy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 290,
       "text": [
        "<matplotlib.text.Text at 0x21aa17b8>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x146ac470>"
       ]
      }
     ],
     "prompt_number": 290
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the training data , so when we run for training data we get maximum accuracy when value for alpha is very low, as there will be no word for which relative frequency is 0, as all those words were already encountered when we were training the sytem. So, when we increase value of alpha probability will decrease.\n",
      "For the dev data , there will be words which are not found in training set thus, as we increase alpha .probability increases till value of alpha goes to 1 or 3, but then alpha over weighs in denominators , thus probability starts to decrease."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Feature Analysis #\n",
      "\n",
      "(_Completing  getTopFeats() - 2 pts, Deliverable 4a - 1pt, 4b - 2 pts, 4c - 2 pts, 4d -5pts . Total 7 pts for CS4650 and 12 pts for CS7650_)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4a**\n",
      "What are the words that are most predictive of positive versus negative text?\n",
      "You can measure this by $\\log \\theta_{pos,n} - \\log \\theta_{neg,n}$ (which is similar to the [likelihood ratio test](http://en.wikipedia.org/wiki/Likelihood-ratio_test)).\n",
      "Use $\\alpha = 1$ from the dev data.\n",
      "\n",
      "List the top five words and their counts for each class. Do the same for the top 5 words that predict negative versus positive.\n",
      "\n",
      "Consider using [operator.itemgetter()](http://docs.python.org/2.7/library/operator.html) for easily sorting dictionaries by their values. See my definition of the argmax function above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getWeights(alpha):\n",
      "    counts = defaultdict(lambda : Counter()) # hint\n",
      "    class_counts = defaultdict(int) # hint\n",
      "    class_wordcounts = defaultdict(int)\n",
      "    with open('dev-imdb.key','r') as fin:\n",
      "        for i,keyline in enumerate(fin):\n",
      "            kvp = keyline.split();\n",
      "            class_counts[kvp[1]]+=1\n",
      "            dataloc = keyline.rstrip().split(' ')[0]\n",
      "            with open(dataloc,'r') as infile:\n",
      "                 for line in infile:\n",
      "                            wordCount(line,counts,kvp[1],class_wordcounts)\n",
      "    weights = defaultdict(int)    # your code here\n",
      "    vocab =  len(counts.keys());\n",
      "    for word in counts.keys():\n",
      "        for label in all_labels:\n",
      "            weights[label,word] = log((counts[word][label] + alpha)/(class_wordcounts[label] + vocab*alpha))\n",
      "    return weights\n",
      "def getTopFeats(weights,class1,class2,K=5):\n",
      "    topFeats = defaultdict(int) \n",
      "    for key in weights.keys():\n",
      "        if(key[0]== class1):\n",
      "           topFeats[key[1]]= topFeats[key[1]]+weights.get(key)\n",
      "        if(key[0]== class2):\n",
      "            topFeats[key[1]]= topFeats[key[1]]-weights.get(key)\n",
      "   \n",
      "    topFeats= sorted(topFeats.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
      "    output = defaultdict(lambda : Counter()) \n",
      "    \n",
      "    for i in range(0,5):\n",
      "        output[topFeats[i][0]]= counts[topFeats[i][0]]\n",
      "       \n",
      "     \n",
      "    return output;\n",
      "    # your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 291
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "#weights_nb_alphas = getWeights(1.0)\n",
      "#print weights_nb_alphas\n",
      "print getTopFeats(weights_nb_alphas[1.0],'POS','NEG')\n",
      "print getTopFeats(weights_nb_alphas[1.0],'NEG','POS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<function <lambda> at 0x0000000023E8E5F8>, {'yokai': Counter({'POS': 19}), 'palma': Counter({'POS': 16}), 'tarzan': Counter({'POS': 20, 'NEU': 3}), 'spock': Counter({'POS': 17}), 'friendship': Counter({'POS': 21, 'NEU': 2})})\n",
        "defaultdict(<function <lambda> at 0x0000000023E8E5F8>, {'awful': Counter({'NEG': 103, 'NEU': 19, 'POS': 4}), 'kibbutz': Counter({'NEG': 17}), 'sellers': Counter({'NEG': 18, 'NEU': 4}), 'carlito': Counter({'NEG': 19}), 'insult': Counter({'NEG': 19})})"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 292
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4b** Now do the same thing for $\\alpha = 100$. Which words look better to you? \n",
      "Which gave better accuracy? \n",
      "Explain what you think is going on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "#weights_nb_alphas[100] = getWeights(100.0)\n",
      "print getTopFeats(weights_nb_alphas[100.0],'POS','NEG')\n",
      "print getTopFeats(weights_nb_alphas[100.0],'NEG','POS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<function <lambda> at 0x00000000149146D8>, {'great': Counter({'POS': 457, 'NEU': 164, 'NEG': 133}), 'wonderful': Counter({'POS': 100, 'NEU': 15, 'NEG': 13}), 'love': Counter({'POS': 284, 'NEG': 123, 'NEU': 71}), 'best': Counter({'POS': 311, 'NEG': 107, 'NEU': 85}), 'excellent': Counter({'POS': 114, 'NEU': 27, 'NEG': 19})})\n",
        "defaultdict(<function <lambda> at 0x00000000149146D8>, {'awful': Counter({'NEG': 103, 'NEU': 19, 'POS': 4}), 'bad': Counter({'NEG': 496, 'NEU': 130, 'POS': 126}), 'waste': Counter({'NEG': 82, 'NEU': 8, 'POS': 7}), 'worst': Counter({'NEG': 169, 'NEU': 25, 'POS': 17}), 'worse': Counter({'NEG': 111, 'POS': 17, 'NEU': 15})})"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 293
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Higher alpha tends to give better accuracy, as now in higher alpha we find words which are have high overall frequency (total count for all of them is atleast greater than 100 ) as compared to alpha =1, where we had words with low frequency. Higher alpha, does now allow words with very low count to get higher probability, which makes sense, as we can derive better results from words which have occured 100 or 1000 times as compared to words which just occur once or twice."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4c** Consider the weights for $\\alpha=100$. \n",
      "\n",
      "- Print words $w$ that are in the positive lexicon, but for which $\\log \\phi_{neg,w} > \\log \\phi_{pos,w} + 0.1$. \n",
      "    - (These words are more likely in the negative class, despite being in the positive lexicon.)\n",
      "- Print words $w$ that are in the negative lexicon, but for which the $\\log \\phi_{pos,w} > \\log \\phi_{neg,w} + 0.1$. \n",
      "    - (These words are more likely in the positive class, despite being in the negative lexicon.)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getTopThresholdWords(weights,class1,class2,K=0.1):\n",
      "    topFeats = defaultdict(int) \n",
      "    for key in weights.keys():\n",
      "        if(key[0]== class1):\n",
      "           topFeats[key[1]]= topFeats[key[1]]+weights.get(key)\n",
      "        if(key[0]== class2):\n",
      "            topFeats[key[1]]= topFeats[key[1]]-weights.get(key)\n",
      "    \n",
      "    topFeats= sorted(topFeats.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
      "    output = defaultdict(lambda : Counter()) \n",
      "    \n",
      "    for i in range(0,len(topFeats)):\n",
      "            if topFeats[i][1] < K:\n",
      "                break\n",
      "            output[topFeats[i][0]]= counts[topFeats[i][0]]\n",
      "    words = []\n",
      "    if class1 == 'POS':\n",
      "        for key in output.keys():\n",
      "            if key in negwords:\n",
      "                words.append(key)\n",
      "    if class1 == 'NEG':\n",
      "        for key in output.keys():\n",
      "            if key in poswords:\n",
      "                words.append(key)\n",
      "\n",
      "    return words;\n",
      "print \"Negative Lexicon\"\n",
      "print getTopThresholdWords(weights_nb_alphas[100.0],'POS','NEG')\n",
      "print \"Positive Lexicon \"\n",
      "print getTopThresholdWords(weights_nb_alphas[100.0],'NEG','POS')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Negative Lexicon\n",
        "['struggle', 'fun', 'haunting', 'grim', 'drama', 'emotional']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Positive Lexicon \n",
        "['just', 'better', 'pretty', 'kind', 'like', 'honestly', 'okay', 'redeeming', 'wonder', 'wow', 'please', 'joke', 'talent', 'want', 'funny']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 294
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4d** (7650 only)\n",
      "\n",
      "What do you think is going on here? Pick one of these words, and look for example reviews that contain it (using [grep](http://en.wikipedia.org/wiki/Grep)). \n",
      "\n",
      "Is the word used in the opposite sense, or is there some other explanation?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(explain here)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Here, some of the words , which are mentioned in positive lexicon , generally turn out to be part of negative sentences/paragraphs. #Lets take the word \"better\" which is a superlative form of a word good. Better is generally used in positive\n",
      "#connotation stating or comparing qualities . However, in some cases it is used to compare better between the negative ones. example: They could have done such a better job, better designs and animatronics and NO VENUS.\n",
      "#Also, sometimes word makes a different sense, when seen in a sentence. example Yes, this film would have been better made if the real opera singers had made this movie.\n",
      "#   Here \"better\" is used with \"if\" stating the negative things. Some more examples are include \"Even Christmas Vacation was better than this\"\n",
      "#Thus, it is the mix of word used in opposite sense as well as sense changes when taken in context of sentence."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 295
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example 1 : Throwing yourself out a window would be better than watching this movie --Here better is used to compare negative alternatives."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 296
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Example 2:  The book gives you a better development of the characters. --Here better is used in the correct sense, however the paragraph is about how movies destroy the story of the book, so whole paragraph is in negative connotation."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 297
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Example 3: Would have been a much better movie if the story was confined more to the kidnap instead of the character failings of the kidnappers. -- This paragraph states that movie was not good and how it could have been made better. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 298
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 298
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}