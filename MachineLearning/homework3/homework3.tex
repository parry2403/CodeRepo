\documentclass[twoside,10pt]{article}
\usepackage{amsmath,amsfonts,amsthm,fullpage}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}


\begin{document}

\title{CS 7641 CSE/ISYE 6740 Homework 3}
\author{Le Song}
\date{Deadline: 11/13 Tue, 11:59pm}
\maketitle

\begin{itemize}
  \item Submit your answers as an electronic copy on T-square.
  \item No unapproved extension of deadline is allowed. Zero credit will be assigned for late submissions. Email request for late submission may not be replied.
  \item For typed answers with LaTeX (recommended) or word processors, extra credits will be given. If you handwrite, try to be clear as much as possible. No credit may be given to unreadable handwriting.
  \item Explicitly mention your collaborators if any.
  \item Recommended reading: PRML\footnote{Christopher M. Bishop, Pattern Recognition and Machine
Learning, 2006, Springer.} Section 3.1, 3.2
\end{itemize}

%----------------------------------------------------------------------------------
\section{Linear Regression [30 pts]}

In class, we derived a closed form solution (normal equation) for
linear regression problem: $\hat{\theta} = (X^T X)^{-1} X^T Y$. A
probabilistic interpretation of linear regression tells us that we
are relying on an assumption that each data point is actually
sampled from a linear hyperplane, with some white noise. The noise
follows a zero-mean Gaussian distribution with constant variance.
Mathematically,
\begin{equation}
Y^i = \theta^T X^i + \epsilon^i
\end{equation}
where $\epsilon^i \sim \mathcal{N}(0, \sigma^2 I)$. In other words,
we are assuming that each data point is independent to each other
(no covariance) and that each data point has same variance.

\subsubsection*{(a) Using the normal equation, derive the expectation
$\mathbb{E}[\hat{\theta}]$. [6 pts]}

\begin{equation}
Y = \theta^T X+ \epsilon
\end{equation}
where $\epsilon^i \sim \mathcal{N}(0, \sigma^2 I)$ \\
This equation basically tells that Y is a combination of Linear function in X and Normal Distributed error with 0 mean and constant variance. \\
Thus we can write this as

\begin{equation}
Y /X\sim \mathcal{N}(\theta^T X, \sigma^2 I)
\end{equation}
In other words, linear regression assumes that given X, Y is normally distributed with mean that is linearly increasing in
X and constant variance. In particular, no assumption is made on the distribution P(X). Thus Y also follows Normal Distribution.\\

Now , we know that
$\hat{\theta} = (X^T X)^{-1} X^T Y$ . We can also see that $\mathbb{E}[\hat{\theta}]$ is basically  $\mathbb{E}[\hat{\theta /X}]$, as calculate this is supervised learning where we calculate / estimate the value of 
$\theta $ with respect to the known inputs X. \\
Note that , equation for variables in matrix form will be of the type //
\begin{equation}
\bold Y /\bold X\sim \mathcal{N}(\bold X \theta , \sigma^2 I)
\end{equation}
\begin{equation} \nonumber
\begin{split}
 \mathbb{E}[\hat{\theta }/X]&= \mathbb{E}[(X^T X)^{-1} X^T Y /X] \\ 
 &=(X^T X)^{-1} X^T \mathbb{E}[Y /X] \\
 &=(X^T X)^{-1} X^T \mathbb{E}[Y /X] \\
 &=(X^T X)^{-1} X^T \bold X \theta \\
&=\theta 
 \end{split}
\end{equation}


\subsubsection*{(b) Using the normal equation, derive the variance $\text{Var}[\hat{\theta}]$. [6 pts]}
This part now, will be similar to what we did above, Thus \\

\begin{equation} \nonumber
\begin{split}
\text{Var}[\hat{\theta }]&= \text{Var}[\hat{\theta }/X]\\
=> \text{Var}[\hat{\theta }/X]&= \text{Var}[(X^T X)^{-1} X^T Y /X] \\ 
 &=(X^T X)^{-1} X^T \text{Var}[Y /X]  ((X^T X)^{-1} X^T )^T\\
 &=(X^T X)^{-1} X^T  \mathbb \sigma^2 I  (X (X^T X)^{-1} ) \\
 &=(X^T X)^{-1}   X^T X\mathbb \sigma^2 (X^T X)^{-1} \\
&=\mathbb \sigma^2 (X^T X)^{-1}\\
 \end{split}
\end{equation}
Note that here we used the fact that $ {(K^{-1})}^T = {(K^{T})}^{-1}$, thus ${((X^T X)^{-1})}^T = (X^T X)^{-1}$ \\ 
We also used that if A is constant then, $Var(AY)= A^2Var(Y) = A*A^TVar(Y)$
\subsubsection*{(c) Under the white noise assumption above, someone claims that $\hat{\theta}$ follows Gaussian
distribution with mean and variance in (a) and (b), respectively. Do
you agree with this claim? Why or why not? [8 pts]}

We know that  MSE is the sum of the bias squared  and the
variance. Since bias is zero here ($E(\theta) - \theta$), we see that if X is $n x d$ matrix then $X^T*X$ is a $d x d$ matrix , whose value increases with increasing the n that is the number of input points. \\
Thus, for  MSE( sum of the bias squared (which is zero) and the
variance )we have that the d × d MSE matrix $\mathbb \sigma^2 (X^T X)^{-1}$
decreases to the zero matrix as n  tends to $ \infty$ (the entries of $(X^T X)$
get bigger as n increases and therefore the entries of $\mathbb \sigma^2 (X^T X)^{-1}$ decrease).\\
 We also see that the MSE is linearly related to
the inherent noise level $\sigma^2$ of ${\epsilon} $ . We have shown above that conditioned on X the vector Y is multivariate normal. Since $\hat{\theta}$ is a linear function of Y
it is also multivariate Gaussian (since a linear transform of a multivariate normal RV is a multivariate normal RV). We
therefore have
\begin{equation}
\hat \theta\sim \mathcal{N}(\theta,\mathbb \sigma^2 (X^T X)^{-1})
\end{equation}
\subsubsection*{(d) Weighted linear regression}

Suppose we keep the independence (no covariance) assumption but
remove the same variance assumption. Then, data points would be
still sampled independently, but now they may have different
variance $\sigma_i$. Thus, the covariance matrix would be still
diagonal, but with different values:
\begin{equation}
\Sigma = \begin{bmatrix}
\sigma_1^2 & 0 & \dots & 0\\
0 & \sigma_2^2& \dots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \dots & \sigma_n^2
\end{bmatrix}.
\end{equation}
Derive the normal equation for this problem using matrix-vector
notations with $\Sigma$. [10 pts]

We know and have derived above that Y /X follows Gaussian Distribution, 
\begin{equation}
\bold Y /\bold X\sim \mathcal{N}(\bold X \theta , \sigma^2 I)
\end{equation}

In this case, as each of the points have different variance, thus we will have equation of the form
\begin{equation}
Y_i /X_i\sim \mathcal{N}(\theta ^T X_i , \sigma_i^2 I)
\end{equation}
Now as each points are iid's , thus probability of Y can be calculated by taking the product of their individual probabilities. \\

Inverse of Covariance Matrix 
 \begin{equation} \nonumber
\begin{split}
\Sigma_{k}^{-1}= 1/ \sigma^2 * I
 \end{split}
\end{equation}, So its only effect is to scale vector product by $1/ \sigma^2$

\begin{equation}
p(Y|x,\theta, \Sigma) = \prod_{i=1}^n \mathcal{N}(\theta ^T X_i , \sigma_i^2 I)
\end{equation}
Putting the value of \begin{equation}
p(Y_i/X_i) = \frac{1}{ (2\pi)^{d/2} \sigma} \exp
\left( - \frac{(Y_i -\theta ^T X_i)^t (Y_i - \theta ^T X_i)}{2\sigma_i^2 } \right).\nonumber
\end{equation}, we get

\begin{equation}
p(Y|x,\theta, \Sigma) = \frac{1}{ (2\pi)^{nd/2} \sigma^n} \exp
\left( \sum_{i=1}^{n}- \frac{(Y_i -\theta ^T X_i)^t (Y_i - \theta ^T X_i)}{2\sigma_i^2 } \right)
\end{equation}
Taking the log 
\begin{equation}
\log p(Y|x,\theta, \Sigma) =\frac {nd}{2} \log \pi + n \log \sigma + \sum_{i=1}^{n}- \frac{(Y_i -\theta ^T X_i)^t (Y_i - \theta ^T X_i)}{2\sigma_i^2 }
\end{equation}
Taking the derivative wrt $ \theta$ and setting it to 0  we get
\begin{equation}
\begin{split}
0&= \sum_{i=1}^{n} \frac{(Y_i -\theta ^T X_i) (X_i)^t }{2\sigma_i^2 } \\
 \sum_{i=1}^{n}  \frac{(Y_i ) (X_i)^t }{2\sigma_i^2 } &=\sum_{i=1}^{n} \frac{(\theta ^T X_i) (X_i)^t }{2\sigma_i^2 }
\end{split}
\end{equation}

Note that \begin{equation}\frac{( X_i) (X_i)^t }{2\sigma_i^2}\end{equation} is basically can be written in vector form as 



\begin{equation}
 [X_1, X_2 .....]\begin{bmatrix}
\sigma_1^2 & 0 & \dots & 0\\
0 & \sigma_2^2& \dots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \dots & \sigma_n^2
\end{bmatrix}\begin{bmatrix}
       X_1         \\[0.3em]
       X_2 \\[0.3em]
       \vdots
     \end{bmatrix}
\end{equation}, Thus in matrix notation , we can write this as \\


\begin{equation} \nonumber
\begin{split}
Y \Sigma^{-1} X^T &= \theta^T X \Sigma^{-1} X^T \\
\theta^T  &= (Y \Sigma^{-1} X^T) (X \Sigma^{-1} X^T)^{-1} \\
\theta &= ((Y \Sigma^{-1} X^T) (X \Sigma^{-1} X^T)^{-1})^T \\
\theta &= ((X \Sigma^{-1} X^T)^{-1})^T (Y \Sigma^{-1} X^T)^T \\
\theta &= (X \Sigma^{-1} X^T)^{-1} (X \Sigma^{-1} Y^T)
\end{split}
\end{equation}
\vspace{1cm}


%----------------------------------------------------------------------------------
\section{Ridge Regression [15 pts]}

For linear regression, it is often assumed that $y = \theta^\top \textbf{x} + \epsilon$ where $\theta, \textbf{x} \in \mathbb{R}^m$ by absorbing the constant term, and $\epsilon \sim \mathcal{N} (0, \sigma^2)$ is a Gaussian random variable. Given $n$ i.i.d samples $(\textbf{x}^1, y^1), ..., (\textbf{x}^n, y^n)$, we define $\textbf{y} = (y^1, ..., y^n)^\top$ and $X = (\textbf{x}^1, ..., \textbf{x}^n)^\top$. Thus, we have $\textbf{y} \sim \mathcal{N} (X\theta, \sigma^2 I)$. Show that the ridge regression estimate is the mean of the posterior distribution under a Gaussian prior $\theta \sim \mathcal{N} (0, \tau^2 I)$. Find the explicit relation between the regularization parameter $\lambda$ in the ridge regression estimate of the parameter $\theta$, and the variances $\sigma^2, \tau^2$.

According to the question , we can write the likelihood of Y
\begin{equation}
\bold Y /\bold X\sim \mathcal{N}(\bold X \theta , \sigma^2 I)
\end{equation}
We are also given the prior 
\begin{equation}
\bold\theta \sim \mathcal{N}(0 , \tau^2 I)
\end{equation}
If we assume that our input data x is
already centered , so that we can ignore the intercept term $\theta_0$. The posterior distribution of ridge regression given Y and X can be written using Bayes theory as:
\begin{equation}
\begin{split}
p(\bold\theta/X,Y) &= \frac{p(Y/X,\theta)*p(\theta)}{Z}\\
\end{split}
\end{equation}
Here Z which is Z(Y,X),is a normalization constant and is independent of $\theta$, thus we can ignore the denominator. \\

Probability of Normal Gaussian Distribution for a general x is given by : \\
\begin{equation}
p(x) = \frac{1}{ (2\pi)^{d/2} \Sigma^{1/2}} \exp
\left( - \frac{(x - \mu)^t \Sigma^{-1}(x - \mu)}{2} \right).\nonumber
\end{equation}

Inverse of Covariance Matrix 
 \begin{equation} \nonumber
\begin{split}
\Sigma_{k}^{-1}= 1/ \sigma^2 * I
 \end{split}
\end{equation}, So its only effect is to scale vector product by $1/ \sigma^2$
\begin{equation}
p(x) = \frac{1}{ (2\pi)^{d/2} \sigma} \exp
\left( - \frac{(x - \mu)^t (x - \mu)}{2\sigma^2 } \right).\nonumber
\end{equation}
Thus after plugging the values in Bayes equation we get the following ,

\begin{equation}
\begin{split}
p(\bold\theta/X,Y) &=\frac{1}{Z}\frac{1}{ (2\pi)^{d/2} \sigma} \exp \left( - \frac{(y - \bold X \theta)^t (y - \bold X \theta)}{2\sigma^2 } \right) * \frac{1}{ (2\pi)^{d/2} \tau} \exp \left( - \frac{( \theta)^t (\theta)}{2\tau^2 } \right)\\
\end{split}
\end{equation}
Taking the log form of posterior distribution, we get \\
\begin{equation}
\begin{split}
\log p(\bold\theta/X,Y) &=-logZ -logK  - \frac{(y - \bold X \theta)^t (y - \bold X \theta)}{2\sigma^2 }  - \frac{( \theta)^t (\theta)}{2\tau^2 } \\
\end{split}
\end{equation}

Here, K corresponds to other terms outside of exp., which are independent of $\theta$. Taking the derivative of this log we get, wrt to $\theta$ and setting it to 0

\begin{equation}
\begin{split}
0 &= \frac{X^t (y - \bold X \theta)}{\sigma^2 }  - \frac{ (\theta)}{\tau^2 } \\
0 &= X^t (y - \bold X \theta)  - \frac{ (\theta)\sigma^2}{\tau^2 } \\
0 &= X^t y - X^t \bold X \theta)  - \frac{ (\theta)\sigma^2}{\tau^2 } \\
0 &= X^t y - (X^t  X  + \frac{ \sigma^2}{\tau^2 } )\theta\\
\theta &=  (X^t  X + \frac{ \sigma^2}{\tau^2 } )^{-1}X^t y\\
\end{split}
\end{equation}
Setting the ridge regression parameter $\lambda = \frac{ \sigma^2}{\tau^2 }$
 , we see that the above solution is equivalent to
ridge regression. When ${\tau^2 }$
is small (meaning we have prior
knowledge to believe $\theta$ is close to zero), then $\lambda $ is large (meaning large $\theta$ will be penalized).
When $ \sigma^2$
is small (meaning that observations y are not noisy), we will focus on fitting the
data (small $\lambda $). \\
 Now, since posterior is product of two gaussians , therefore it will also be gaussian. Let $m_b$ and $\Sigma_b$,  be its mean and variance, then its exponent will be , \\
Expansion of squared Mahalanobis distance. \\
\begin{equation} 
\begin{split}
(\theta - m_b)^t \Sigma_{b}^{-1}(\theta  - m_b)\\
=&\theta ^t \Sigma_{b}^{-1}\theta  -2(\Sigma_{b}^{-1}m_b)^t \theta  + m_b ^t\Sigma_{b}^{-1}m_b  
 \end{split}
\end{equation}
Equating the second order $\theta$ terms between eq 11 and 13 we get 

\begin{equation}
\begin{split}
\theta ^t \Sigma_{b}^{-1}\theta=& \frac{ \theta ^t X^tX\theta}{\sigma^2} +\frac{\theta ^t \theta}{\tau^2} \\
 \Sigma_{b}^{-1}=& \frac{1}{\sigma^2}(X^tX + \frac{ \sigma^2}{\tau^2 } I) \\
 \end{split}
\end{equation}

Then you can show by equating the value of $\theta$ ,we can obtain the mean by plugging in
\begin{equation} 
\begin{split}
m_{b}=& (X^tX + \frac{ \sigma^2}{\tau^2 } I )^{-1}  X^t y\\
 =& \frac{\Sigma_{b}X^t y}{\sigma^2}\\
  \end{split}
\end{equation}
\vspace{1cm}

%----------------------------------------------------------------------------------
\section{Bias - Variance Decomposition [15 pts]}
Suppose $x$ is a $d$-dimensional vector. The estimator $S_j^2$ defined as 
$$S_j^2=\frac{1}{n-1}\sum_{i=1}^n (\textbf{x}^i_j-\bar{\textbf{x}}_j)^2$$
where $i=1,\dots,n$, $j=1,\dots,d$ and $\bar{\textbf{x}}_j=\frac{1}{n} \sum_{i=1}^n \textbf{x}^i_j$, is used to estimate the diagonal of covariance matrix, that is, $diag(Cov(X))$. Show that $S_j^2$ is an unbiased estimator.

\vspace{1cm}
Estimator is said to be unbiased  $E(\hat{\theta}) = \theta$ 
Thus ,to prove that an estimator $\hat{\theta}$ for the parameter $\theta$ is unbiased, we need to show that the bias of $\hat{\theta}$ is $Bias(\hat{\theta}) = E(\hat{\theta}) - \theta = 0$, \\
Now, iwe are given that $x$ is a $d$-dimensional vector, and $S_j^2$ is the estimator which is used to estimate the diagonal of covariance matrix, that is, $diag(Cov(X_j))$. Here, $X = \{x_j^i\}$ where $i = 1,\ldots, n\ \&\ j = 1,\ldots,d$. \\
Finally, we need to show that $S_j^2$ is an unbiased estimator. \\

Thus, we need to show that $E(S_j^2) = diag(Cov(X_j))$ where $j = 1,\ldots,d$, ie $E(S^2) = ES^2 = diag(Cov(X))$. \\

We are given that:

\begin{equation} \nonumber
\begin{split}
\bar{\textbf{x}}_j &= \frac{1}{n} \sum_{i=1}^n \textbf{x}^i_j \\
E(\bar{\textbf{x}}_j) &= E(\frac{1}{n} \sum_{i=1}^n \textbf{x}^i_j)  \\
&= \sum_{i=1}^n \frac{E(\textbf{x}^i_j)}{n} \\
&= \frac{n E(\textbf{x}_j)}{n} 
= E(\textbf{x}_j)
\end{split}
\end{equation}


Let us consider  the following result:

\begin{equation} \nonumber
\begin{split}
\sum_{i=1}^n (\textbf{x}^i_j-\bar{\textbf{x}}_j)^2 &= \sum_{i=1}^n (\textbf{x}^i_j)^2 - 2\bar{\textbf{x}}_j \sum_{i=1}^n \textbf{x}^i_j + n\bar{\textbf{x}}_j^2 \\
&= \sum_{i=1}^n (\textbf{x}^i_j)^2 - 2n\bar{\textbf{x}}_j\bar{\textbf{x}}_j + n\bar{\textbf{x}}_j^2 \\
&= \sum_{i=1}^n (\textbf{x}^i_j)^2 - n\bar{\textbf{x}}_j^2
\end{split}
\end{equation}

To prove that $S^2$ is unbiased we will show that it is unbiased in the one dimensional case i.e., $S_j^2$ are scalars (if this holds, we can apply this result to each component separately to get unbiasedness of the vector $S^2$).\\

Therefore using the result from above,

\begin{equation} 
\begin{split}
E\left(\sum_{i=1}^n (\textbf{x}^i_j-\bar{\textbf{x}}_j)^2\right) &= E\left(\sum_{i=1}^n (\textbf{x}^i_j)^2 - n\bar{\textbf{x}}_j^2\right) \\
&= \sum_{i=1}^n E((\textbf{x}^i_j)^2) - nE(\bar{\textbf{x}}_j^2) 
= n E((\textbf{x}_j)^2) - nE(\bar{\textbf{x}}_j^2)
\end{split}
\end{equation}

Now, using the relations between variance and Expectations know that:

\begin{equation} \nonumber
\begin{split}
Var(X) &= E(X^2) - (E(X))^2 \\
E(X^2) &= Var(X) + (EX)^2
\end{split}
\end{equation}

Similarly, we get:

\begin{equation} \nonumber
\begin{split}
Var(\bar{X}) &= E(\bar{X}^2) - (E(\bar{X}))^2 \\
E(\bar{X}^2) &= \frac{Var(X)}{n} + (E\bar{X})^2
\end{split}
\end{equation}

Now, substituting the expectations $E(X^2)$ and $E(\bar{X}^2)$, we will get:

\begin{equation} \nonumber
\begin{split}
E\left(\sum_{i=1}^n (\textbf{x}^i_j-\bar{\textbf{x}}_j)^2\right) &= n E((\textbf{x}_j)^2) - nE(\bar{\textbf{x}}_j^2) \\
&= n\left(Var(X) + (EX)^2\right) - n\left(\frac{Var(X)}{n} + (E\bar{X})^2\right) \\
&= (n-1)Var(X) \\
\frac{E\left(\sum_{i=1}^n (\textbf{x}^i_j-\bar{\textbf{x}}_j)^2\right)}{n-1} &= Var(X) \\
E\left(\frac{\sum_{i=1}^n (\textbf{x}^i_j-\bar{\textbf{x}}_j)^2}{n-1}\right) &= Var(X) \\
E(S_j^2) &= diag(Cov(X_j))
\end{split}
\end{equation}

Now, similarly to the multivariate case, this implies $E(S_j^2) =  diag(Cov(X_j))$ for all $j = 1,.......,d$, and therefore:

\begin{equation} 
ES^2 = diag(Cov(X))
\end{equation}

As Expectation is just equal to the variance, thus it proves that the $S^2$ is unbiased.




%----------------------------------------------------------------------------------
\section{Programming: Recommendation System [40 pts]}

Personalized recommendation systems are used in a wide variety of
applications such as electronic commerce, social networks, web
search, and more. Machine learning techniques play a key role to
extract individual preference over items. In this assignment, we
explore this popular business application of machine learning, by
implementing a simple matrix-factorization-based recommender using
gradient descent.

Suppose you are an employee in Netflix. You are given a set of
ratings (from one star to five stars) from users on many movies they
have seen. Using this information, your job is implementing a
personalized rating predictor for a given user on unseen movies.
That is, a rating predictor can be seen as a function $f:
\mathcal{U} \times \mathcal{I} \rightarrow \mathbb{R}$, where
$\mathcal{U}$ and $\mathcal{I}$ are the set of users and items,
respectively. Typically the range of this function is restricted to
between 1 and 5 (stars), which is the the allowed range of the
input.

Now, let's think about the data representation. Suppose we have $m$
users and $n$ items, and a rating given by a user on a movie. We can
represent this information as a form of matrix, namely rating matrix
$M$. Suppose rows of $M$ represent users, while columns do movies.
Then, the size of matrix will be $m \times n$. Each cell of the
matrix may contain a rating on a movie by a user. In $M_{15,47}$,
for example, it may contain a rating on the item 47 by user 15. If
he gave 4 stars, $M_{15,47} = 4$. However, as it is almost
impossible for everyone to watch large portion of movies in the
market, this rating matrix should be very sparse in nature.
Typically, only 1\% of the cells in the rating matrix are observed
in average. All other 99\% are missing values, which means the
corresponding user did not see (or just did not provide the rating
for) the corresponding movie. Our goal with the rating predictor is
estimating those missing values, reflecting the user's preference
learned from available ratings.

Our approach for this problem is matrix factorization. Specifically,
we assume that the rating matrix $M$ is a low-rank matrix.
Intuitively, this reflects our assumption that there is only a small
number of factors (e.g, genre, director, main actor/actress,
released year, etc.) that determine like or dislike. Let's define
$r$ as the number of factors. Then, we learn a user profile $U \in
\mathbb{R}^{m \times r}$ and an item profile $V \in \mathbb{R}^{n
\times r}$. (Recall that $m$ and $n$ are the number of users and
items, respectively.) We want to approximate a rating by an inner
product of two length $r$ vectors, one representing user profile and
the other item profile. Mathematically, a rating by user $u$ on
movie $i$ is approximated by
\begin{equation}
M_{u,i} \approx \sum_{k=1}^r U_{u,k} V_{i,k}.
\end{equation}
We want to fit each element of $U$ and $V$ by minimizing squared
reconstruction error over all training data points. That is, the
objective function we minimize is given by
\begin{equation}
E(U,V) = \sum_{(u, i) \in M} (M_{u,i} - U_u^T V_i)^2 = \sum_{(u, i)
\in M} (M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k})^2 \label{eq:svd}
\end{equation}
where $U_u$ is the $u$th row of $U$ and $V_i$ is the $i$th row of
$V$. We observe that this looks very similar to the linear
regression, which we learned in the class (from slide 24 in lecture
15). Recall that we minimize in linear regression:
\begin{equation}
E(\theta) = \sum_{i=1}^m (Y^i - \theta^T x^i)^2 = \sum_{i=1}^m (Y^i
- \sum_{k=1}^r \theta_k x^i_k)^2 \label{eq:linear_reg}
\end{equation}
where $m$ is the number of training data points. Let's compare
\eqref{eq:svd} and \eqref{eq:linear_reg}. $M_{u,i}$ in
\eqref{eq:svd} corresponds to $Y^i$ in \eqref{eq:linear_reg}, in
that both are the observed labels. $U_u^T V_i$ in \eqref{eq:svd}
corresponds to $\theta^T x^i$ in \eqref{eq:linear_reg}, in that both
are our estimation with our model. The only difference is that both
$U$ and $V$ are the parameters to be learned in \eqref{eq:svd},
while only $\theta$ is learned in \eqref{eq:linear_reg}. This is
where we personalize our estimation: with linear regression, we
apply the same $\theta$ to any input $x^i$, but with matrix
factorization, a different profile $U_u$ are applied depending on
who is the user $u$.

As $U$ and $V$ are interrelated in \eqref{eq:svd}, there is no
closed form solution, unlike linear regression case. Thus, we need
to use gradient descent:
\begin{equation}
U_{v,k} \gets U_{v,k} - \mu \frac{\partial E(U, V)}{\partial
U_{v,k}}, \quad \quad V_{j,k} \gets V_{j,k} - \mu \frac{\partial
E(U, V)}{\partial V_{j,k}},\label{eq:gd}
\end{equation}
where $\mu$ is a hyper-parameter deciding the update rate. It would
be straightforward to take partial derivatives of $E(U,V)$ in
\eqref{eq:svd} with respect to each element $U_{v,k}$ and $V_{j,k}$.
Then, we update each element of $U$ and $V$ using the gradient
descent formula in \eqref{eq:gd}.


\subsubsection*{(a) Derive the update formula in \eqref{eq:gd} by
solving the partial derivatives. [10 pts]}

We know that:

\begin{equation} \nonumber
\begin{split}
E(U,V) &= \sum_{(u, i) \in M} (M_{u,i} - U_u^T V_i)^2 \\
&= \sum_{(u, i) \in M} (M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k})^2 \label{eq:svd}
\end{split}
\end{equation}

By Calculating the partial derivatives wrt to $U_{v,k}$: \\

$\frac{\partial E(U,V)}{\partial U_{v,k}}$ is given by: 

\begin{equation} \nonumber
\begin{split}
\frac{\partial E(U,V)}{\partial U_{v,k}} &= \frac{\partial}{\partial U_{v,k}}\left(\sum_{(u, i) \in M} (M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k})^2\right) \\
&= \sum_{(u, i) \in M} \frac{\partial}{\partial U_{v,k}} \left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right)^2 \\
&= \sum_{(i:M(u,i) > 0)} 2\left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) \frac{\partial}{\partial U_{v,k}} \left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) \\
&= \sum_{(i:M(u,i) > 0)} 2\left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) \left( \frac{\partial M_{u,i}}{\partial U_{v,k}}  - \frac{\partial}{\partial U_{v,k}} \sum_{k=1}^r U_{u,k} V_{i,k}\right) \\
&= \sum_{(i:M(u,i) > 0)} 2\left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) \left(0 - V_{i,k}\right) \\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) V_{i,k} \\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) V_{i,k} \\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) V_{i,k} = -2 (\textbf{M} - \textbf{U}\textbf{V}^T) \textbf{V}
\end{split}
\end{equation}

$\frac{\partial E(U,V)}{\partial V_{j,k}}$ is given by:

\begin{equation} \nonumber
\begin{split}
\frac{\partial E(U,V)}{\partial V_{j,k}} &= \frac{\partial}{\partial V_{j,k}}\left(\sum_{(u, i) \in M} (M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k})^2\right) \\
&= \sum_{(u, i) \in M} \frac{\partial}{\partial V_{j,k}} \left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right)^2 \\
&= \sum_{(i:M(u,i) > 0)} 2\left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) \left( \frac{\partial M_{u,i}}{\partial V_{j,k}}  - \frac{\partial}{\partial V_{j,k}} \sum_{k=1}^r U_{u,k} V_{i,k}\right) \\
&= \sum_{(i:M(u,i) > 0)} 2\left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) \left(0 - U_{u,k}\right) \\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) U_{u,k} \\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k} \\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k} 
= -2 {(\textbf{M} - \textbf{U}\textbf{V}^T)}^T \textbf{U}
\end{split}
\end{equation}

So,  update rules for  $U_{v,k} \& V_{j,k}$ as follows: 

\begin{equation} \nonumber
\begin{split}
U_{v,k} &= U_{v,k} - \mu \frac{\partial E(U, V)}{\partial U_{v,k}} \\
&= U_{v,k} - \mu \left( -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) V_{i,k} \right) \\
&= U_{v,k} + 2 \mu \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) V_{i,k}
\end{split}
\end{equation}

\begin{equation} \nonumber
\begin{split}
V_{j,k} &= V_{j,k} - \mu \frac{\partial E(U, V)}{\partial V_{j,k}} \\
&= V_{j,k} - \mu \left( -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k}\right) \\
&= V_{j,k} + 2 \mu \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k}
\end{split}
\end{equation}

\subsubsection*{(b) To avoid overfitting, we usually add regularization terms, which penalize for large values in $U$ and $V$.
Redo part (a) using the regularized objective function below. [5
pts]}

\begin{equation}
E(U,V) = \sum_{(u, i) \in M} (M_{u,i} - \sum_{k=1}^r U_{u,k}
V_{i,k})^2 + \lambda \sum_{u,k} U_{u,k}^2 + \lambda \sum_{i,k}
V_{i,k}^2 \nonumber
\end{equation}

($\lambda$ is a hyper-parameter controlling the degree of
penalization.)

Now, we will calculate the partial derivatives as follows: \\

$\frac{\partial E(U,V)}{\partial U_{v,k}}$ is given by:  (Using the partial derivative results from the first  part , we get) \\

\begin{equation} \nonumber
\begin{split}
\frac{\partial E(U,V)}{\partial U_{v,k}} &= \frac{\partial}{\partial U_{v,k}}\left(\sum_{(u, i) \in M} (M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k})^2 + \lambda \sum_{u,k} U_{u,k}^2 + \lambda \sum_{i,k} V_{i,k}^2\right) \\
&= \frac{\partial}{\partial U_{v,k}}\left(\sum_{(u, i) \in M} (M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k})^2\right) + \frac{\partial}{\partial U_{v,k}} \left( \lambda \sum_{u,k} U_{u,k}^2 \right) + \frac{\partial}{\partial U_{v,k}} \left( \lambda \sum_{i,k} V_{i,k}^2 \right) \\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k} +
\lambda \sum_{u,k} \frac{\partial}{\partial U_{v,k}} \left( U_{u,k}^2 \right) + \lambda \sum_{i,k} \frac{\partial}{\partial U_{v,k}} \left( V_{i,k}^2 \right) \\
&= \sum_{(i:M(u,i) > 0)} 2\left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) \left(0 - V_{i,k}\right) + 2 \lambda U_{u,k}\\
&= -2 (\textbf{M} - \textbf{U}\textbf{V}^T) \textbf{V} + 2 \lambda \textbf{U}
\end{split}
\end{equation}

$\frac{\partial E(U,V)}{\partial V_{j,k}}$ is given by:

\begin{equation} \nonumber
\begin{split}
\frac{\partial E(U,V)}{\partial V_{j,k}} &= \frac{\partial}{\partial V_{j,k}}\left(\sum_{(u, i) \in M} (M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k})^2 + \lambda \sum_{u,k} U_{u,k}^2 + \lambda \sum_{i,k} V_{i,k}^2\right) \\
&= \frac{\partial}{\partial V_{j,k}}\left(\sum_{(u, i) \in M} (M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k})^2\right) + \frac{\partial}{\partial V_{j,k}} \left( \lambda \sum_{u,k} U_{u,k}^2 \right) + \frac{\partial}{\partial V_{j,k}} \left( \lambda \sum_{i,k} V_{i,k}^2 \right) \\
&= \sum_{(i:M(u,i) > 0)} 2\left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) \left(0 - U_{u,k}\right) + 2 \lambda V_{j,k} \\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - \sum_{k=1}^r U_{u,k} V_{i,k}\right) U_{u,k} + 2 \lambda V_{j,k}\\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k} + 2 \lambda V_{j,k}\\
&= -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k} + 2 \lambda V_{j,k}\\
&= -2 {(\textbf{M} - \textbf{U}\textbf{V}^T)}^T \textbf{U} + 2 \lambda \textbf{V}
\end{split}
\end{equation}

So, now that we have found the gradients, we will update $U_{v,k} \& V_{j,k}$ as follows: 

\begin{equation} \nonumber
\begin{split}
U_{v,k} &= U_{v,k} - \mu \frac{\partial E(U, V)}{\partial U_{v,k}} \\
&= U_{v,k} - \mu \left( -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) V_{i,k} + 2 \lambda U_{u,k} \right) \\
&= (1-2\lambda)U_{v,k} + 2 \mu \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) V_{i,k}
\end{split}
\end{equation}

\begin{equation} \nonumber
\begin{split}
V_{j,k} &= V_{j,k} - \mu \frac{\partial E(U, V)}{\partial V_{j,k}} \\
&= V_{j,k} - \mu \left( -2 \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k} + 2 \lambda V_{j,k}\right) \\
&= (1-2\lambda)V_{j,k} + 2 \mu \sum_{(i:M(u,i) > 0)} \left(M_{u,i} - U_u^T V_i\right) U_{u,k}
\end{split}
\end{equation}


\subsubsection*{(c) Implement \texttt{myRecommender.m} by filling the gradient descent part.}

You are given a skeleton code \texttt{myRecommender.m}. Using the
training data \texttt{rateMatrix}, you will implement your own
recommendation system of rank \texttt{lowRank}. The only file you
need to edit and submit is \texttt{myRecommender.m}. In the gradient descent
part, repeat your update formula in (b), observing the average
reconstruction error between your estimation and ground truth in
training set. You need to set a stopping criteria, based on this
reconstruction error as well as the maximum number of iterations.
You should play with several different values for $\mu$ and
$\lambda$ to make sure that your final prediction is accurate.

Formatting information is here:

\subsubsection*{Input}
\begin{itemize}
  \item \textbf{rateMatrix}: training data set. Each row represents a user, while each column an item.
  Observed values are one of $\{1,2,3,4,5\}$, and missing values are
  0.
  \item \textbf{lowRank}: the number of factors (dimension) of your
  model. With higher values, you would expect more accurate
  prediction.
\end{itemize}

\subsubsection*{Output}
\begin{itemize}
  \item \textbf{U}: the user profile matrix of dimension user count
  $\times$ low rank.
  \item \textbf{V}: the item profile matrix of dimension item count
  $\times$ low rank.
\end{itemize}

\subsubsection*{Evaluation [15 pts]}
Upload your \texttt{myRecommender.m} implementation file. (Do not
copy and paste your code in your report. Be sure to upload your
\texttt{myRecommender.m} file.)

To test your code, try to run \texttt{homework3.m}. You may have
noticed that the code prints both training and test error, in RMSE
(Root Mean Squared Error), defined as follows:
\begin{equation}
\sum_{(u,i) \in M} (M_{u,i} - f(u,i))^2 \nonumber
\end{equation}
where $f(u,i)$ is your estimation, and the summation is over the
training set or testing set, respectively. For the grading, we will
use another set-aside testing set, which is not released to you. If
you observe your test error is less than 1.00 without cheating (that
is, directly referring to the test set), you may expect to see the
similar performance on the unseen test set as well.

Note that we provide \texttt{homework3.m} just to help you evaluate your code easily. You are not expected to alter or submit this to us. In other words, we will not use this file when we grade your submission. The only file we take care of is \texttt{myRecommender3.m}.

Grading criteria:
\begin{itemize}
  \item Your code should output $U$ and $V$ as specified. The dimension should match to the specification.
  \item We will test your output on another test dataset, which was not provided to you. The test RMSE on this dataset should be at least 1.05 to get at least partial credit.
  \item We will measure elapsed time for learning. If your implementation takes longer than 3 minutes for rank 5, you should definitely try to make your code faster or adjust parameters. Any code running more than 5 minutes is not eligible for credit.
  \item Your code should not crash. Any crashing code will be not credited.
\end{itemize}

\subsubsection*{Report [10 pts]}
In your report, show the performance (RMSE) both on your training
set and test set, with varied \texttt{lowRank}. (The default is set
to 1, 3, and 5, but you may want to vary it further.) Discuss what
you observe with varied low rank. Also, briefly discuss how you
decided your hyper-parameters ($\mu, \lambda$).

\subsubsection*{Discussion}
Performance (RMSE) for the following svd values are:
I am able to get  RMSE around.92-.93 for SVD3 and SVD5. Time taken is around 30 seconds per SVD. 
Insert a Table here.\\ [.25cm]
\begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    SVD & TrainRMSE & DevRMSE & Time \\ \hline
   SVD-1	&  0.9171 & 0.9474 &	36.81
     \\ \hline
   SVD-3	& 0.8664 &	0.9349 &	38.42\\ \hline
    SVD-5	& 0.8393 &	0.9360 &	38.59 \\ \hline
    SVD-7 & 0.8266 &  0.9354 &	41.96\\
    \hline
     SVD-9	& 0.8140 &	0.9288 &	40.71 \\
    \hline
    \end{tabular}\\[.25cm]
Discuss \\
I had tried two approaches and depending on that I took different values of $\mu, \lambda$. In the first case, I kept   learningRate = 0.00025;     regularizer = 0.03 . In this case I kept on decreasing the value of learning rate marginally as I was getting more and more close to the data ,but kept the regularizer same. I was getting around .93 to .94 for both SVD3 and SVD5. (code in myRecommendation1.m). Its needs to be noted that RMSE on training data decreased as we increased the SVD number. It also decreased as we increased the number of iteration, but it had not impact for improvement in DevRMSE. Thus it was doing overfitting on test data. I tried to combat this by increasing regularizer,not in every iteration but it every 100 iterations.This approach was run 450 times or when the error decreases below a bound.Values for this approach are as follows.\\[.25cm]
SVD-1	0.9171	0.9474	36.81 \\
bhatia, parminder\\
SVD-3	0.8664	0.9349	38.42\\
bhatia, parminder\\
SVD-5	0.8393	0.9360	38.59\\
bhatia, parminder\\
SVD-7	0.8266	0.9354	41.96\\
bhatia, parminder\\
SVD-9	0.8140	0.9288	40.71\\[.25cm]
Next approach, which I used is Bolt Driver approach, where we keep on increasing learning rate  by 1.2 times till the time error is decreasing and as soon as we find an increase in error, we decrement the learning rate by half. We initialized the values for learningRate = 0.000003   regularizer = 3 . Note that this approach reduced the RMSE for test data very well but was overfitting and performance and time consumed were slightly higher than the earlier approach.\\
Bhatia, Parminder\\
SVD-1	0.9222	0.9554	49.58\\
Bhatia, Parminder\\
SVD-3	0.8479	0.9280	52.64\\
Bhatia, Parminder\\
SVD-5	0.8061	0.9357	52.87\\
Bhatia, Parminder\\
SVD-7	0.7655	0.9457	52.37\\
Bhatia, Parminder\\
SVD-9	0.7376	0.9634	52.90\\






\subsubsection*{Note}
\begin{itemize}
  \item Do not print anything in your code (e.g, iteration 1 :
  err=2.4382) in your final submission.
  \item Do not alter input and output format of the skeleton file. (E.g, adding a new parameter without specifying its defalut value) Please make sure that you returned all necessary outputs according to the given skeleton.
  \item Please do not use additional file. This task is simple enough that you can fit in just one file.
  \item Submit your code with the best parameters you found. We will grade without
modifying your code. (Applying cross-validation to find best
parameters is fine, though you do not required to do.)
  \item Please be sure that your program finishes within a fixed number of
iterations. Always think of a case where your stopping criteria is
not satisfied forever. This can happen anytime depending on the
data, not because your code is incorrect. For this, we recommend
setting a maximum number of iteration in addition to other stopping
criteria.
\end{itemize}

\subsubsection*{Grand Prize}

Similar to the Netflix competition held in 2006 to 2009, the student who achives the lowest RMSE on the secret test set will earn the Grand Prize. We will give extra 10 bonus points to the winner, and share the student's code to everyone. (Note that the winner should satisfy all other grading criteria perfectly, including answer sanity check and timing requirement. Otherwise, the next student will be considered as the winner.)

\subsubsection*{Typing Bonus}

As usual, we will give 5 bonus points for typed submissions. Note that \textbf{all} questions should be typed to get this credit.

%\bibliographystyle{plain}
%\bibliography{temp,externalPapers,groupPapers}

\end{document}
